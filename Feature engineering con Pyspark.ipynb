{"cells":[{"cell_type":"markdown","source":["# Feature Engineering con PySpark\n**Por: Naren Castellon**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"82f31e42-9693-4da7-bcf9-66a1c2de57b9","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<center><img src=\"https://www.hogarmania.com/archivos/202209/curiosidades-del-oso-panda-1280x720x80xX.jpg \" width=\"550\" height=\"850\"></center>\n\n#![](files/image.jpg)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"730857fd-9c43-4018-a9d3-bc643253bbcc","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Este modulo vamos a cubrir los patrones de diseño para trabajar con características de datos (cualquier atributo medible, desde precios de automóviles hasta valores genéticos, recuentos de hemoglobina o niveles educativos) al crear modelos de **Machine learning** (también conocido como ingeniería de características). Estos procesos (extracción, transformación y selección de características) son esenciales para construir modelos efectivos de **Machine learning**. La ingeniería de características es una de las más importantes temas en el **Machine learning**, porque el éxito o el fracaso de un modelo en la predicción del futuro depende principalmente de las funciones que elija.\n\n**Spark** proporciona una API integral de **Machine learning** para muchos algoritmos conocidos, incluidos la *regresión lineal, la regresión logística y los árboles de decisión*. El objetivo de este modulo es presentar herramientas y técnicas fundamentales en **PySpark** que puede usar para crear todo tipo de **pipelines** de **Machine learning**. Vamos a presentar las poderosas herramientas y utilidades de **Machine learning** de Spark y proporciona ejemplos usando la API de PySpark. Las habilidades que aprenda aquí serán útiles para un aspirante a científico de datos o ingeniero de datos. Mi objetivo este cuaderno no es familiarizarlo con los famosos algoritmos de aprendizaje automático, como la regresión lineal, el análisis de componentes principales o las máquinas de vectores de soporte, ya que estos lo vamos a cubrir en otro modulo, sino equiparlo con herramientas (normalización, estandarización, indexación de cadenas, etc. .) que puede usar para limpiar datos y crear modelos para una amplia gama de algoritmos de **Machine learning**.\n\nIndependientemente del algoritmo que vayamos a utilizar, la ingeniería de características es importante. En **Machine learning** nos permite encontrar patrones en los datos: encontramos los patrones construyendo modelos, luego usamos los modelos construidos para hacer predicciones sobre nuevos puntos de datos (es decir, consultar datos). Para obtener esas predicciones correctas, debemos construir el conjunto de datos y transformar los datos correctamente. Este capítulo cubre estos dos pasos clave."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5a5e6f5c-b2b7-4a7f-9920-3378847d53cc","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# Contenido\n\n* Agregar nuevas caracteristicas\n* Creando y aplicando UDFs\n* Creando pipelines\n* Binarizing data\n* Imputación de Datos\n* Tokenization\n* Estandarización\n* Normalizacion\n* String indexing\n* Vector assembly\n* Bucketing\n* Logarithm transformation\n* One-hot encoding\n* TF-IDF\n* Feature hashing\n* Applying SQL transformations"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"42a946a3-96ac-4018-8425-321db010bce4","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Introducción a Feature Engineering\n**Feature Engineering** es la forma de definir *\"el proceso de transformar datos sin procesar en funciones que representen mejor el problema subyacente de los modelos predictivos, lo que da como resultado una mayor precisión del modelo en datos ocultos\"*. En este apartado, mi objetivo es presentar técnicas genéricas de ingeniería de características disponibles en PySpark que puede usar para crear mejores modelos predictivos.\n\nDigamos que sus datos están representados en una matriz de filas y columnas. En **Machine Learning**, las columnas se denominan características (como edad, sexo, educación, frecuencia cardíaca o presión arterial) y cada fila representa una instancia del conjunto de datos (es decir, un registro). Las características de sus datos influirán directamente en los modelos predictivos que cree y use y en los resultados que pueda lograr. Los científicos de datos dedicamos alrededor de la mitad de nuestro tiempo en la preparación de datos, y la ingeniería de características es una parte importante de esto.\n\n¿Dónde encaja la **ingeniería de características** con la construcción de modelos de **Machine Learning**? ¿Cuándo aplica estas técnicas a sus datos? Echemos un vistazo a los pasos clave para construir y usar un modelo de aprendizaje automático:\n\n1. Reúna los requisitos para los datos de **Machine Learning** y defina el problema.\n2. Seleccionar datos (recopilar e integrar los datos, luego desnormalizarlos en un conjunto de datos).\n3. Preprocesar datos (formatear, limpiar y muestrear los datos para poder trabajar con ellos).\n4. Transformar datos (realizar ingeniería de características).\n5. Datos del modelo (divida los datos en conjuntos de entrenamiento y prueba, use los datos de entrenamiento para crear modelos, luego use los datos de prueba para evaluar los modelos y ajustarlos).\n6. Use el modelo construido para hacer predicciones sobre los datos de la consulta."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c50dc469-dc30-4520-b9dc-8b2b75e5039d","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["**Feature Engineering** ocurre justo antes de construir un modelo a partir de sus datos. Después de seleccionar y limpiar los datos (por ejemplo, asegurarse de que los valores nulos se reemplacen con los valores adecuados), transforme los datos realizando ingeniería de características: esto podría implicar convertir cadenas en datos numéricos, categorizar los datos, normalizar o estandarizar los datos, etc."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"41682848-ccc4-4d58-9a00-e89d4f0bad3f","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<center><img src=\"https://assets-global.website-files.com/620d42e86cb8ec4d0839e59d/6230e9ee021b250dd3710f8e_61ca4fbcc80819e696ba0ee9_Feature-Engineering-Machine-Learning-Diagram.png\" width=\"500\" height=\"500\"></center>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"addfb52e-5b36-418a-9de5-8c9253d2765b","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["La API de Spark proporciona varios algoritmos para trabajar con funciones, que se dividen aproximadamente en estos grupos:\n* Extracción (algoritmos para extraer características de datos \"sin procesar\")\n* Transformación (algoritmos para escalar, convertir o modificar características)\n* Selección (algoritmos para seleccionar un subconjunto de un conjunto más grande de características)\n* Hashing sensible a la localidad (LSH); algoritmos para agrupar elementos similares)\n\nPuede haber muchas razones para la transformación de datos y la ingeniería de funciones, ya sean obligatorias u opcionales:\n\nTransformaciones obligatorias\n\nEstas transformaciones son necesarias para resolver un problema (como construir un modelo de **Machine Learning** por razones de compatibilidad de datos. Ejemplos incluyen:\n* Conversión de características no numéricas en características numéricas. Por ejemplo, si una característica tiene valores no numéricos, los cálculos de promedio, suma y mediana serán imposibles; del mismo modo, no podemos realizar la multiplicación de matrices en una cadena, sino que primero debemos convertirla en alguna representación numérica.\n* Cambiar el tamaño de las entradas a un tamaño fijo. Algunos modelos lineales y redes neuronales feed-forward tienen un número fijo de nodos de entrada, por lo que sus datos de entrada siempre deben tener el mismo tamaño. Por ejemplo, los modelos de imagen necesitan remodelar las imágenes en su conjunto de datos a un tamaño fijo.\n\nTransformaciones opcionales\n\nLas transformaciones de datos opcionales pueden ayudar a que el modelo de **Machine Learning** funcione mejor. Estas transformaciones pueden incluir:\n* Cambiar el texto a minúsculas antes de aplicar otras transformaciones de datos\n* Tokenización y eliminación de palabras no esenciales, como \"de\", \"un\", \"y\", \"el\" y \"entonces\"\n* Normalización de características numéricas\n\nExaminaremos ambos tipos en las siguientes secciones. Profundicemos en nuestro primer tema, agregando una nueva característica."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"12fc1ea9-772f-4939-af7b-89e9b13617bc","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Agregar nuevas caracteristicas\nA veces deseamos agregar una nueva característica (porque necesita esa característica derivada en su algoritmo de **Machine Learning**) en el conjunto de datos, para agregar una nueva columna o característica en el conjunto de datos, se puede usar la función `DataFrame.withColumn()`. Este concepto se demuestra a continuación:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fe81730b-24ea-40d1-9320-b777f8cfc363","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# importar SparkSession\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import *\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cc2074dd-f48f-4575-9403-82bae2244854","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# crear objeto de sesión spark\nspark=SparkSession.builder.appName(\"Feature engineering\").getOrCreate()\n    "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5eb2ac50-7807-4bc6-bba1-c86a60e009d0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["column_names = [\"emp_id\", \"salario\"]\nrecords = [(100, 120000), (200, 170000), (300, 150000)]\ndf = spark.createDataFrame(records, schema=column_names)\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"940b88bb-d72d-4d9d-9896-18a6dfbfddd5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------+-------+\n|emp_id|salario|\n+------+-------+\n|   100| 120000|\n|   200| 170000|\n|   300| 150000|\n+------+-------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["Puede usar `DataFrame.withColumn()` de Spark para agregar una nueva columna/característica:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a8f0ba6d-e027-49cf-8faf-742dbe9e6bc3","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df2 = df.withColumn(\"bonus\", df.salario * 0.05)\ndf2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5912f6dc-6889-4af5-b501-e5996ff128c5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------+-------+------+\n|emp_id|salario| bonus|\n+------+-------+------+\n|   100| 120000|6000.0|\n|   200| 170000|8500.0|\n|   300| 150000|7500.0|\n+------+-------+------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["## Creando y aplicando UDFs\n\n**¿Qué es UDF?**\nLas funciones definidas por el usuario de UDF, también conocidas como funciones definidas por el usuario, si proviene de SQL, las UDF no son nada nuevo para usted, ya que la mayoría de las bases de datos RDBMS tradicionales admiten funciones definidas por el usuario, estas funciones deben registrarse en la biblioteca de la base de datos y usarlas en SQL como funciones regulares.\n\nLas UDF de PySpark son similares a las UDF de las bases de datos tradicionales. En PySpark, crea una función en una sintaxis de Python y la envuelve con PySpark SQL udf() o la registra como udf y la usa en DataFrame y SQL respectivamente.\nSi PySpark no proporciona la función que necesita, puede definir sus propias funciones de Python y registrarlas como funciones definidas por el usuario (UDF) con el DSL de Spark SQL usando `spark.udf.register()`. Luego puede aplicar estas funciones en sus transformaciones de datos.\n\nPara que sus funciones de Python sean compatibles con los marcos de datos de Spark, debe convertirlas en UDF de PySpark pasándolas a la función `pyspark.sql.func tions.udf()`. Alternativamente, puede crear su UDF en un solo paso usando anotaciones, como se muestra aquí. Agregue udf@ como un \"decorador\" de su función de Python y especifique su tipo de devolución como argumento:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"aa157e28-1286-43c4-9c40-e9de8ee28f14","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["**¿Por qué necesitamos un UDF?**\n\nLos UDF se utilizan para ampliar las funciones del marco y reutilizar estas funciones en múltiples DataFrame. Por ejemplo, desea convertir cada primera letra de una palabra en una cadena de nombre a mayúsculas; Las funciones integradas de PySpark no tienen esta función, por lo tanto, puede crearla como UDF y reutilizarla según sea necesario en muchos marcos de datos. Una vez creados, los UDF se pueden reutilizar en varios DataFrame y expresiones SQL.\n\nAntes de crear cualquier UDF, investigue para verificar si la función similar que deseaba ya está disponible en Spark SQL Functions. PySpark SQL proporciona varias funciones comunes predefinidas y se agregan muchas más funciones nuevas con cada versión. por lo tanto, es mejor verificar antes de reinventar la rueda.\n\nCuando crea UDF, debe diseñarlos con mucho cuidado; de lo contrario, se encontrará con problemas de optimización y rendimiento."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d84731c9-b183-4799-9274-3463704877a2","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["La función `tripled()` es una UDF y su tipo de retorno es entero."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"448c08bb-96ec-4eb5-87ca-80db56d73e71","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import udf\n@udf(\"integer\")\ndef tripled(num):\n    return 3*int(num)\n\ndf2 = df.withColumn('tripled_col', tripled(df.salario))\ndf2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"df440df1-ad71-4463-b032-fd6d56dfc33a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------+-------+-----------+\n|emp_id|salario|tripled_col|\n+------+-------+-----------+\n|   100| 120000|     360000|\n|   200| 170000|     510000|\n|   300| 150000|     450000|\n+------+-------+-----------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### Crear un UDF PySpark \n#### Creando un DataFrame\nAntes de comenzar a crear un UDF, primero creemos un PySpark DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b8fff4af-3e89-4ea8-8cad-03be48427257","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ncolumns = [\"Seqno\",\"Nombre\"]\ndata = [(\"1\", \"juan Jones\"),\n    (\"2\", \"tracey aguilar\"),\n    (\"3\", \"amy castellon\")]\n\ndf = spark.createDataFrame(data=data,schema=columns)\n\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cc277d69-4137-441c-8c52-aefe4e5dd54a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+--------------+\n|Seqno|Nombre        |\n+-----+--------------+\n|1    |juan Jones    |\n|2    |tracey aguilar|\n|3    |amy castellon |\n+-----+--------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### Crear una función de Python\nEl primer paso para crear una UDF es crear una función de Python. El siguiente fragmento crea una función `convertCase()` que toma un parámetro de cadena y convierte la primera letra de cada palabra en mayúscula. Las UDF toman los parámetros de su elección y devuelven un valor."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bcb653e3-5cbc-4e5a-997f-cd3b1f7f1153","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def convertCase(str):\n    resStr=\"\"\n    arr = str.split(\" \")\n    for x in arr:\n       resStr= resStr + x[0:1].upper() + x[1:len(x)] + \" \"\n    return resStr \n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"56f97299-7c8b-4553-ad93-d2f2728681df","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Tenga en cuenta que podría haber una mejor manera de escribir esta función. Pero por el bien de este artículo, no me preocupa mucho el rendimiento y las mejores formas."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5aebeeba-01d1-4abd-978f-ec62c080a95a","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Convertir una función de Python a PySpark UDF\nAhora convierta esta función convertCase() a UDF pasando la función a PySpark SQL udf(), esta función está disponible en el paquete org.apache.spark.sql.functions.udf. Asegúrese de importar este paquete antes de usarlo.\n\nLa función PySpark SQL udf() devuelve el objeto de clase org.apache.spark.sql.expressions.UserDefinedFunction."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8679480a-b584-45f0-9a81-a5e026e9e4a3","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import StringType\n\n# Convertiendo la función a UDF \nconvertUDF = udf(lambda z: convertCase(z),StringType())\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"568d2bd7-aaf5-49a8-ba2f-d3adab776496","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[15]: <function __main__.<lambda>(z)>"]}],"execution_count":0},{"cell_type":"markdown","source":["Nota: El tipo predeterminado de udf() es StringType, por lo tanto, también puede escribir la declaración anterior sin tipo de retorno."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"acb69eb6-f16c-450c-b357-ecb178d684a4","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Usando UDF con DataFrame\n#### Usando UDF con PySpark DataFrame select()\nAhora puede usar `convertUDF()` en una columna DataFrame como una función incorporada normal."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"12d22bd3-f98c-449e-8828-5f9d8718e2a8","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.select(col(\"Seqno\"), \\\n    convertUDF(col(\"nombre\")).alias(\"Name\") ) \\\n   .show(truncate=False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f88ff833-c4ab-4047-8b40-067b0ba384a2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+---------------+\n|Seqno|Name           |\n+-----+---------------+\n|1    |Juan Jones     |\n|2    |Tracey Aguilar |\n|3    |Amy Castellon  |\n+-----+---------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["Tenga en cuenta que si sus funciones se representan como un RDD (donde cada elemento de RDD representa una instancia de sus funciones), puede usar la función `RDD.map()` para agregar una nueva función a su conjunto de funciones.\n\nhttps://sparkbyexamples.com/pyspark/pyspark-udf-user-defined-function/"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0121acbc-a4ee-43e2-a64a-7a5844e3ada5","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Creando Pipelines\nEn los algoritmos de **Machine Learning**, puede unirse varias etapas y ejecutarlas en orden. Considere tres etapas, llamadas {Etapa-1, Etapa-2, Etapa-3}, donde la salida de la Etapa-1 se usa como entrada para la Etapa-2 y la salida de la Etapa-2 se usa como entrada para la Etapa- 3. Estas tres etapas forman una tubería simple. Supongamos que tenemos que transformar los datos en el orden que se muestra en la Tabla\n\n|Stage| Description|\n|-----|------------|\n|Stage-1 |Label encode or string index the column dept * (create dept_index column).\n|Stage-2 |Label encode or string index the column education (create education_index column).\n|Stage-3| One-hot encode the indexed column education_index (create education_OHE column)."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bdbe384f-528d-4f4f-822a-9cf4e816099f","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<center><img src=\"https://www.qubole.com/wp-content/uploads/2018/12/image15.png\" width=\"550\" height=\"850\"></center>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6d4c6c5f-9909-4a2b-938b-9536fe1b21c4","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Spark proporciona una API de **Pipeline**, definida como `pyspark.ml.Pipeline(*,stags=None)`, que actúa como un estimador (una abstracción de un algoritmo de aprendizaje que se ajusta a un modelo en un conjunto de datos). Según la documentación de Spark:\n> Un Pipeline se especifica como una secuencia de etapas, y cada etapa es un Transformador o un Estimador. Estas etapas se ejecutan en orden y el DataFrame de entrada se transforma a medida que pasa por cada etapa. Para las etapas de Transformer, se llama al método `transform()` en el DataFrame. Para las etapas de Estimator, se llama al método `fit()` para producir un Transformador (que se convierte en parte de PipelineModel, o Pipeline ajustado), y se llama al método `transform()` de ese Transformador en el DataFrame.\n\n>\n<center><img src=\"https://spark.apache.org/docs/latest/img/ml-Pipeline.png\" width=\"550\" height=\"850\"></center>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"20ddfa54-2284-43b1-9655-ef094e52e38b","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["> Arriba, la fila superior representa un Pipeline con tres etapas. Los dos primeros (Tokenizer y HashingTF) son Transformers (azul) y el tercero (LogisticRegression) es un Estimator (rojo). La fila inferior representa los datos que fluyen a través de la canalización, donde los cilindros indican tramas de datos. Se llama al método Pipeline.fit() en el DataFrame original, que tiene etiquetas y documentos de texto sin formato. El método Tokenizer.transform() divide los documentos de texto sin procesar en palabras, agregando una nueva columna con palabras al DataFrame. El método HashingTF.transform() convierte la columna de palabras en vectores de características, agregando una nueva columna con esos vectores al DataFrame. Ahora, dado que LogisticRegression es un Estimador, Pipeline primero llama a LogisticRegression.fit() para producir un LogisticRegressionModel. Si Pipeline tuviera más Estimadores, llamaría al método transform() de LogisticRegressionModel en el DataFrame antes de pasar el DataFrame a la siguiente etapa.\n\n> Un Pipeline es un Estimador. Por lo tanto, después de que se ejecuta el método fit() de Pipeline, produce un PipelineModel, que es un Transformador. Este PipelineModel se usa en el momento de la prueba; la siguiente figura ilustra este uso.\n\n<center><img src=\"https://spark.apache.org/docs/latest/img/ml-PipelineModel.png\" width=\"550\" height=\"850\"></center>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4f09ad48-5003-4efb-b4f9-a66a9cf1f77b","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["En la figura anterior, el `PipelineModel` tiene la misma cantidad de etapas que el Pipeline original, pero todos los Estimadores en el Pipeline original se han convertido en Transformadores. Cuando se llama al método `transform()` de PipelineModel en un conjunto de datos de prueba, los datos se pasan a través de la canalización ajustada en orden. El método `transform()` de cada etapa actualiza el conjunto de datos y lo pasa a la siguiente etapa.\n\n`Pipelines` y `PipelineModels` ayudan a garantizar que los datos de entrenamiento y prueba pasen por pasos de procesamiento de características idénticos.\n\nPara ilustrar el concepto de **Pipelines**, primero crearemos un marco de datos de muestra con tres columnas para usar como datos de entrada, como se muestra aquí, luego crearemos una canalización simple usando`pyspark.ml.Pipeline()`:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"23932757-1a3c-4830-8a82-888ecc10a179","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# spark: Una instancia en SparkSession\n# creamos un DataFrame\ndf = spark.createDataFrame([\n(1, 'CS', 'MS'),\n(2, 'MATH', 'PHD'),\n(3, 'MATH', 'MS'),\n(4, 'CS', 'MS'),\n(5, 'CS', 'PHD'),\n(6, 'ECON', 'BS'), (7, 'ECON', 'BS'),], ['id', 'dept', 'education'])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1e8f07a0-e3ab-4b7d-8513-ba0a8f5b024d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Podemos ver nuestros datos de muestra con `df.show()`:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9ac9523d-fc68-4fb1-91a4-36ad2fe8ecc7","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cab5faea-d9ff-4e21-91a0-8bcf83c76ab2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+----+---------+\n| id|dept|education|\n+---+----+---------+\n|  1|  CS|       MS|\n|  2|MATH|      PHD|\n|  3|MATH|       MS|\n|  4|  CS|       MS|\n|  5|  CS|      PHD|\n|  6|ECON|       BS|\n|  7|ECON|       BS|\n+---+----+---------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["Ahora que hemos creado el DataFrame, supongamos que queremos transformar los datos a través de tres etapas definidas, {etapa_1, etapa_2, etapa_3}. En cada etapa, pasaremos los nombres de las columnas de entrada y salida, y configuraremos el **Pipeline** pasando las etapas definidas al objeto **Pipeline** como una lista.\n\nEl modelo de **Pipeline** de Spark luego realiza pasos específicos uno por uno en una secuencia y nos brinda el resultado final deseado."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"19ce4672-fe6c-40e8-a15f-7f0bac451f25","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.ml.feature import OneHotEncoder"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f0a05b17-59a2-4294-b7f4-075f3a5445e6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Stage 1: transforma el `dept` columna a numerica\nstage_1 = StringIndexer(inputCol= 'dept', outputCol= 'dept_index')\n#\n# Stage 2: transforma la `education` a columna numérica\nstage_2 = StringIndexer(inputCol= 'education', outputCol= 'education_index')\n#\n# Stage 3: one-hot encode the numeric column `education_index`\nstage_3 = OneHotEncoder(inputCols=['education_index'],\noutputCols=['education_OHE'])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b4a03149-9102-4b5d-ab66-a02b2e927e26","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["A continuación, definiremos nuestro pipeline con estas tres etapas:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3854a2f5-afd3-401f-a84e-d0c72a90df60","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# configurar la canalización: pegar las etapas juntas\npipeline = Pipeline(stages=[stage_1, stage_2, stage_3])\n\n# ajuste el modelo de tubería y transforme los datos como se define\npipeline_model = pipeline.fit(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fee4881e-9e39-40bc-a8c2-f15bfa94f717","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# view the transformed data\nfinal_df = pipeline_model.transform(df)\nfinal_df.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eebfbe3b-3287-409f-83c6-62eb4945b1f6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+----+---------+----------+---------------+-------------+\n|id |dept|education|dept_index|education_index|education_OHE|\n+---+----+---------+----------+---------------+-------------+\n|1  |CS  |MS       |0.0       |0.0            |(2,[0],[1.0])|\n|2  |MATH|PHD      |2.0       |2.0            |(2,[],[])    |\n|3  |MATH|MS       |2.0       |0.0            |(2,[0],[1.0])|\n|4  |CS  |MS       |0.0       |0.0            |(2,[0],[1.0])|\n|5  |CS  |PHD      |0.0       |2.0            |(2,[],[])    |\n|6  |ECON|BS       |1.0       |1.0            |(2,[1],[1.0])|\n|7  |ECON|BS       |1.0       |1.0            |(2,[1],[1.0])|\n+---+----+---------+----------+---------------+-------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["## Binarizing Data\nBinarizar los datos significa establecer los valores de las características en 0 o 1 según algún umbral. Los valores mayores que el umbral se asignan a 1, mientras que los valores menores o iguales al umbral se asignan a 0. Con el umbral predeterminado de 0, solo los valores positivos se asignan a 1. La binarización es, por lo tanto, el proceso de umbralización de características numéricas a binario {0, 1} características.\n\n`Binarizer()` de Spark toma los parámetros *inputCol* y *outputCol*, así como el umbral para la binarización. Los valores de características superiores al umbral se binarizan a 1,0; los valores iguales o inferiores al umbral se binarizan a 0,0. \n\nPrimero, creemos un DataFrame con una sola función:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f6a60f47-272c-4d35-8952-d6fb1b7e186e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import Binarizer\nraw_df = spark.createDataFrame([\n(1, 0.1),\n(2, 0.2),\n(3, 0.5),\n(4, 0.8),\n(5, 0.9),\n(6, 1.1)\n], [\"id\", \"feature\"])\n\nraw_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6436d02b-c896-4154-9ee4-689423d636ca","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-------+\n| id|feature|\n+---+-------+\n|  1|    0.1|\n|  2|    0.2|\n|  3|    0.5|\n|  4|    0.8|\n|  5|    0.9|\n|  6|    1.1|\n+---+-------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["A continuación, crearemos un Binarizer con `threshold=0.5`, por lo que cualquier valor inferior o igual a 0.5 se asignará a 0.0 y cualquier valor superior a 0.5 se asignará a 1.0:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8f940f5d-bdf3-483b-97b1-4f11869d2ebf","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import Binarizer\nbinarizer = Binarizer(threshold=0.5, inputCol=\"feature\",outputCol=\"binarized_feature\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2580a7e8-0b65-4a6a-bec3-69d0bf69fb59","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Finalmente, aplicamos el Binarizer definido a una columna de características:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"06e25a60-4233-496f-8933-4a7cb0e9a162","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["binarized_df = binarizer.transform(raw_df)\nprint(\"Salida del binarizador con Threshold = %f\" % binarizer.getThreshold())\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ce5e90fb-37af-4519-8e26-bde9da114e95","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Salida del binarizador con Threshold = 0.500000\n"]}],"execution_count":0},{"cell_type":"code","source":["#binarized_df = binarizer.transform(raw_df)\nbinarized_df.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"aa127a03-1e0d-4c32-a5fd-47ccbfc03fd3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-------+-----------------+\n|id |feature|binarized_feature|\n+---+-------+-----------------+\n|1  |0.1    |0.0              |\n|2  |0.2    |0.0              |\n|3  |0.5    |0.0              |\n|4  |0.8    |1.0              |\n|5  |0.9    |1.0              |\n|6  |1.1    |1.0              |\n+---+-------+-----------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["## Imputación de Datos\nImputer de Spark's es un transformador de imputación para completar los valores faltantes. Los conjuntos de datos del mundo real suelen contener valores faltantes, a menudo codificados como valores nulos, espacios en blanco, **NaNs** u otros marcadores de posición. Hay muchos métodos para manejar estos valores, incluidos los siguientes:\n* Elimine instancias si falta alguna característica (esto podría no ser una buena idea ya que se perderá información importante de otras características).\n* Para una característica faltante, encuentre el valor promedio de esa característica y complete ese valor.\n* Imputar los valores faltantes (es decir, inferirlos de la parte conocida de los datos).\n\nEsta suele ser la mejor estrategia."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ff94ef86-b771-495d-85f1-3809c7c76436","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["`class pyspark.ml.feature.Imputer(*, strategy='mean', missingValue=nan, \ninputCols=None, outputCols=None,inputCol=None, outputCol=None, relativeError=0.001)`"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a84db81f-d19d-417d-8e90-2675abee810c","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Utiliza la media o la mediana de las columnas en las que se encuentran los valores faltantes. Las columnas de entrada deben ser de tipo numérico; actualmente `Imputer` no admite funciones categóricas y puede crear valores incorrectos para una función categórica.\n\nTenga en cuenta que el valor de la media/mediana/moda se calcula después de filtrar los valores faltantes. Todos los valores nulos en las columnas de entrada se tratan como perdidos y, por lo tanto, también se imputan. Para calcular la mediana, se utiliza la función `pyspark.sql.DataFrame.approxQuantile()` con un error relativo de 0,001.\n\nPuede indicarle al imputador que impute valores personalizados que no sean **NaN** usando ``.set MissingValue(custom_value)``. Por ejemplo, .setMissingValue(0) le indica que impute todas las apariciones de 0 (nuevamente, los valores nulos en las columnas de entrada se tratarán como faltantes y también se imputarán).\n\nEl siguiente ejemplo muestra cómo se puede utilizar un imputer. Supongamos que tenemos un DataFrame con tres columnas, id, col1 y col2:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0552760b-6d1c-4be5-96a2-675faeb13546","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df = spark.createDataFrame([(1, 12.0, 5.0),(2, 7.0, 10.0),(3, 10.0, 12.0),(4, 5.0, float(\"nan\")),(5, 6.0, None),\n                            (6, float(\"nan\"), float(\"nan\")),(7, None, None)], [\"id\", \"col1\", \"col2\"])\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ada6e701-8252-4161-aaf3-3a6f442c0d6b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+----+----+\n|id |col1|col2|\n+---+----+----+\n|1  |12.0|5.0 |\n|2  |7.0 |10.0|\n|3  |10.0|12.0|\n|4  |5.0 |NaN |\n|5  |6.0 |null|\n|6  |NaN |NaN |\n|7  |null|null|\n+---+----+----+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["A continuación, creemos un imputer y apliquémoslo a nuestros datos creados:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c23961d3-8de6-46fa-a0b1-a1984c4161b7","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import Imputer\nimputer = Imputer(inputCols=[\"col1\", \"col2\"],outputCols=[\"col1_out\", \"col2_out\"])\n\n# Realiza la imputación\nmodel = imputer.fit(df)\n\ntransformed = model.transform(df)\n\n# Muestra el resultado\ntransformed.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"48835a57-c224-4a60-848f-c42e28a7de39","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+----+----+--------+--------+\n|id |col1|col2|col1_out|col2_out|\n+---+----+----+--------+--------+\n|1  |12.0|5.0 |12.0    |5.0     |\n|2  |7.0 |10.0|7.0     |10.0    |\n|3  |10.0|12.0|10.0    |12.0    |\n|4  |5.0 |NaN |5.0     |9.0     |\n|5  |6.0 |null|6.0     |9.0     |\n|6  |NaN |NaN |8.0     |9.0     |\n|7  |null|null|8.0     |9.0     |\n+---+----+----+--------+--------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["¿Cómo conseguimos los números para los valores faltantes (8,0 para col1 y 9,0 para col2)? Es fácil; dado que la estrategia predeterminada es \"media\", simplemente calculamos los promedios para cada columna y los usamos para los valores faltantes:\n\n$$col1: (12.0+7.0+10.0+5.0+6.0) / 5 = 40 / 5 = 8.0$$\n$$col2: (5.0+10.0+12.0) / 3 = 27.0 / 3 = 9.0$$\n\nEn función de sus requisitos de datos, es posible que desee utilizar una estrategia diferente para completar los valores que faltan. Puede indicarle a la computadora que use la mediana de los valores de características disponibles en su lugar de la siguiente manera:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"78812ba8-cb64-48e6-8820-6652a992d63e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Estrategia con la mediana\nimputer.setStrategy(\"median\")\n\n# Rellena con la mediana\nmodel = imputer.fit(df)\ntransformed = model.transform(df)\n\n# Muestra el resultado\ntransformed.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9ee59d1b-1c63-4402-b106-3dcac79e8312","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+----+----+--------+--------+\n|id |col1|col2|col1_out|col2_out|\n+---+----+----+--------+--------+\n|1  |12.0|5.0 |12.0    |5.0     |\n|2  |7.0 |10.0|7.0     |10.0    |\n|3  |10.0|12.0|10.0    |12.0    |\n|4  |5.0 |NaN |5.0     |10.0    |\n|5  |6.0 |null|6.0     |10.0    |\n|6  |NaN |NaN |7.0     |10.0    |\n|7  |null|null|7.0     |10.0    |\n+---+----+----+--------+--------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["Para obtener estos valores (7,0 para col1 y 10,0 para col2), solo calculamos el valor de la mediana para cada columna:\n\nmedian(col1) =\nmedian(12.0, 7.0, 10.0, 5.0, 6.0) =\nmedian(5.0, 6.0, 7.0, 10.0, 12.0) =\n7.0\n\nmedian(col2) =\nmedian(5.0, 10.0, 12.0) =\n10.0"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b1128eb8-ad53-4f37-949d-e75feb861ee8","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Tokenization\nLos algoritmos de tokenización se utilizan para dividir una frase, una oración, un párrafo o un documento de texto completo en unidades más pequeñas, como palabras individuales, bigramas o términos. Estas unidades más pequeñas se llaman fichas. Por ejemplo, el analizador léxico (un algoritmo utilizado en la escritura del compilador) divide el código de programación en una serie de tokens eliminando cualquier espacio en blanco o comentarios. Por lo tanto, puede pensar en la tokenización de manera más general como el proceso de dividir una cadena en cualquier tipo de tokens significativos.\n\nEn Spark, puede usar `Tokenizer` y `RegexTokenizer` (que le permite definir estrategias de tokenización personalizadas a través de expresiones regulares) para tokenizar cadenas."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d73c4c4f-4e64-4a50-b2de-d71242ff7731","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Tokenizer\nSpark's Tokenizer es un tokenizador que convierte la cadena de entrada a minúsculas y luego la divide por espacios en blanco. Para mostrar cómo funciona esto, creemos algunos datos de muestra:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"62111dcf-aa72-4727-a158-a340dbed8bf7","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import Tokenizer"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"74b61935-dcab-4aee-a3e3-9f113d251cf8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["docs = [(1, \"a Fox jumped over FOX\"),(2, \"RED of fox jumped\")]\ndf = spark.createDataFrame(docs, [\"id\", \"text\"])\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"66b7d8ba-f302-4661-b426-268731ba65f1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+---------------------+\n|id |text                 |\n+---+---------------------+\n|1  |a Fox jumped over FOX|\n|2  |RED of fox jumped    |\n+---+---------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["Luego aplica el Tokenizer:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ed02d332-bd16-48ea-b1ee-41175a02f75b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Le decimos cual es la columna o variable que desea Tokenizer y la salida\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n\n# Se realiza el Tokenizer\ntokenized = tokenizer.transform(df)\n\ntokenized.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"46212a51-fc30-4027-90d2-809520cb30ec","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+---------------------+---------------------------+\n|id |text                 |tokens                     |\n+---+---------------------+---------------------------+\n|1  |a Fox jumped over FOX|[a, fox, jumped, over, fox]|\n|2  |RED of fox jumped    |[red, of, fox, jumped]     |\n+---+---------------------+---------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import IntegerType\n\ncountTokens = udf(lambda words: len(words), IntegerType())\ntokenized.select(\"text\", \"tokens\").withColumn(\"tokens_length\",countTokens(col(\"tokens\"))).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b61642d6-80eb-4fb4-9065-024bb0519b45","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------------------+---------------------------+-------------+\n|text                 |tokens                     |tokens_length|\n+---------------------+---------------------------+-------------+\n|a Fox jumped over FOX|[a, fox, jumped, over, fox]|5            |\n|RED of fox jumped    |[red, of, fox, jumped]     |4            |\n+---------------------+---------------------------+-------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### RegexTokenizer\nRegexTokenizer de Spark es un tokenizador basado en expresiones regulares que extrae tokens usando el patrón de expresiones regulares proporcionado para dividir el texto (el valor predeterminado) o haciendo coincidir repetidamente la expresión regular (si el parámetro de espacios opcional, que es Verdadero de manera predeterminada, es Falso). Aquí hay un ejemplo:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"da3b5f49-4d99-49bb-a39b-69d4d095b4bf","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import Tokenizer, RegexTokenizer\n\nregexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens\",\npattern=\"\\\\W\", minTokenLength=3)\nregex_tokenized = regexTokenizer.transform(df)\nregex_tokenized.select(\"text\", \"tokens\").withColumn(\"tokens_length\", countTokens(col(\"tokens\"))).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b0cc7e00-8718-40f6-89fd-4769ba805117","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------------------+------------------------+-------------+\n|text                 |tokens                  |tokens_length|\n+---------------------+------------------------+-------------+\n|a Fox jumped over FOX|[fox, jumped, over, fox]|4            |\n|RED of fox jumped    |[red, fox, jumped]      |3            |\n+---------------------+------------------------+-------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### Tokenization con Pipeline\nTambién podemos realizar la tokenización como parte de un pipeline. Aquí, creamos un DataFrame con dos columnas:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2ca6c533-f3a7-4add-9800-71791c6c046c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["docs = [(1, \"a Fox jumped, over, the fence?\"),(2, \"a RED, of fox?\")]\ndf = spark.createDataFrame(docs, [\"id\", \"text\"])\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"80c3093c-cbc0-4adc-99db-71968b8dcb96","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+------------------------------+\n|id |text                          |\n+---+------------------------------+\n|1  |a Fox jumped, over, the fence?|\n|2  |a RED, of fox?                |\n+---+------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["A continuación, aplicamos la función `RegexTokenizer()` a este DataFrame:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b3cbec33-2e21-4b1e-b285-45e51615c94c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import StopWordsRemover\n\ntk = RegexTokenizer(pattern=r'(?:\\p{Punct}|\\s)+', inputCol=\"text\",outputCol='text2')\nsw = StopWordsRemover(inputCol='text2', outputCol='text3')\npipeline = Pipeline(stages=[tk, sw])\ndf4 = pipeline.fit(df).transform(df)\ndf4.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d2455c29-b131-4192-a13c-5d0beb8eed3d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+------------------------------+----------------------------------+--------------------+\n|id |text                          |text2                             |text3               |\n+---+------------------------------+----------------------------------+--------------------+\n|1  |a Fox jumped, over, the fence?|[a, fox, jumped, over, the, fence]|[fox, jumped, fence]|\n|2  |a RED, of fox?                |[a, red, of, fox]                 |[red, fox]          |\n+---+------------------------------+----------------------------------+--------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["## Estandarización\nUna de las técnicas más populares para escalar datos numéricos antes de construir un modelo es la estandarización. La estandarización de un conjunto de datos implica volver a escalar la distribución de valores para que la media de los valores observados (como característica) sea 0,00 y la desviación estándar sea 1,00.\n\nMuchos algoritmos de aprendizaje automático funcionan mejor cuando las variables de entrada numéricas (características) se escalan a un rango estándar. Por ejemplo, los algoritmos como la regresión lineal que usan una suma ponderada de la entrada y los algoritmos como los k-vecinos más cercanos que usan medidas de distancia requieren valores estandarizados, ya que, de lo contrario, los modelos construidos podrían ajustarse por debajo o por encima de los datos de entrenamiento y tener un rendimiento inferior.\n\nUn valor se estandariza de la siguiente manera:\n\n$$y = \\frac{(x – mean)}{ standard_deviation}$$\n\nDonde la media se calcula como:\n\n$$mean = sum(x) / count(x)$$\n\n$$\\hat x=\\frac{1}{N} \\sum_{i=1}^{N} x_i$$\n\nY la desviación estándar se calcula como:\n\n$$standard_deviation = \\sqrt(sum( (x – mean)^2 )/count(x))$$\n\n$$sd=\\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\bar x)^2} $$"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b8afd23d-6f43-4062-b17a-9096b4da4f51","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Por ejemplo, si $X = (1, 3, 6, 10)$, la media/promedio se calcula como:\n\n$$mean = (1+2+6+10)/4 = 20/4 = 5.0$$\n\ny la desviación estándar se calcula como:\n\ndesviación estándar\n\n$$= \\sqrt {(((1-5)^2 + (3-5)^2 + (6-5)^2 + (10-5)^2)) / 4)}$$\n$$= \\sqrt {((16+4+1+25)/4)}$$\n$$= \\sqrt{(46/4)}$$\n$$= \\sqrt(11.5) = 3.39116$$"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c89b4023-ae52-47b5-9bf8-50447b015861","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Entonces, los nuevos valores estandarizados serán:\n\n$$y = (y_1, y_2, y_3, y_4) = (-1.1795, -0.5897, 0.2948, 1.4744)$$\n\ndonde\n$$y_1 = (1 – 5.0) / 3.39116$$\n$$y_2 = (3 - 5.0) / 3.39116$$\n$$y_3 = (6 - 5.0) / 3.39116$$\n$$y_4 = (10 - 5.0) / 3.39116$$\n\nComo puede ver, la media de los valores estandarizados (y) es 0,00 y la desviación estándar es 1,00.\n\nRepasemos cómo realizar la estandarización en PySpark. Digamos que estamos tratando de estandarizar (media = 0.00, stddev = 1.00) una columna en un DataFrame. Primero crearemos un DataFrame de muestra, luego le mostraré dos formas de estandarizar la columna de edad:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"150ea324-9918-49cc-8fd4-643dd09b45b7","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["features = [('alex', 1), ('jans', 3), ('ali', 6), ('bruno', 10)]\ncolumns = (\"nombre\", \"edad\")\nsamples = spark.createDataFrame(features, columns)\nsamples.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2098477d-aa0e-42a8-9647-1ef18c08ca9b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------+----+\n|nombre|edad|\n+------+----+\n|  alex|   1|\n|  jans|   3|\n|   ali|   6|\n| bruno|  10|\n+------+----+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["**El método 1:** es usar funciones de DataFrame:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"61d92938-54fd-4b67-a3e7-ebc667d417da","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import stddev, mean, col\n\n(samples.select(mean(\"edad\").alias(\"mean_edad\"),\n                stddev(\"edad\").alias(\"stddev_edad\")).crossJoin(samples).withColumn(\"edad_scaled\",\n                                                                                 (col(\"edad\") - col(\"mean_edad\")) / col(\"stddev_edad\"))) .show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c25f8050-3f39-4742-bee6-f2e5c1dd9c0b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+------------------+------+----+-------------------+\n|mean_edad|stddev_edad       |nombre|edad|edad_scaled        |\n+---------+------------------+------+----+-------------------+\n|5.0      |3.9157800414902435|alex  |1   |-1.0215078369104984|\n|5.0      |3.9157800414902435|jans  |3   |-0.5107539184552492|\n|5.0      |3.9157800414902435|ali   |6   |0.2553769592276246 |\n|5.0      |3.9157800414902435|bruno |10  |1.276884796138123  |\n+---------+------------------+------+----+-------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["o alternativamente, podemos escribir esto como:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7ca89ed0-1d6f-4bd0-8045-9c5c816ecbda","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["mean_age, sttdev_age = samples.select(mean(\"edad\"), stddev(\"edad\")).first()\nsamples.withColumn(\"edad_scaled\",(col(\"edad\") - mean_age) / sttdev_age).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6446f8cd-436b-42a5-83e9-13d0e1579469","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------+----+-------------------+\n|nombre|edad|edad_scaled        |\n+------+----+-------------------+\n|alex  |1   |-1.0215078369104984|\n|jans  |3   |-0.5107539184552492|\n|ali   |6   |0.2553769592276246 |\n|bruno |10  |1.276884796138123  |\n+------+----+-------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["**Método 2** es usar funciones del paquete ml de PySpark. Aquí, usamos `pyspark.ml.feature.VectorAssembler()` para transformar la columna de edad en un vector, luego estandarizamos los valores con `StandardScaler` de Spark:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"018c2a55-5128-4c00-bc8f-11faacd9eae5","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import StandardScaler\n\nvecAssembler = VectorAssembler(inputCols=['edad'], outputCol=\"edad_vector\")\nsamples2 = vecAssembler.transform(samples)\nsamples2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"96e34006-ddc3-461b-84e5-f07cc7e38400","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------+----+-----------+\n|nombre|edad|edad_vector|\n+------+----+-----------+\n|  alex|   1|      [1.0]|\n|  jans|   3|      [3.0]|\n|   ali|   6|      [6.0]|\n| bruno|  10|     [10.0]|\n+------+----+-----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["scaler = StandardScaler(inputCol=\"edad_vector\", outputCol=\"edad_scaled\",withStd=True, withMean=True)\nscalerModel = scaler.fit(samples2)\nscaledData = scalerModel.transform(samples2)\nscaledData.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4b0555d0-06b0-4b99-ac21-08a7d6f4c6c0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------+----+-----------+---------------------+\n|nombre|edad|edad_vector|edad_scaled          |\n+------+----+-----------+---------------------+\n|alex  |1   |[1.0]      |[-1.0215078369104984]|\n|jans  |3   |[3.0]      |[-0.5107539184552492]|\n|ali   |6   |[6.0]      |[0.2553769592276246] |\n|bruno |10  |[10.0]     |[1.276884796138123]  |\n+------+----+-----------+---------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["A diferencia de la normalización, que veremos a continuación, la estandarización puede ser útil en los casos en que los datos siguen una distribución gaussiana. Tampoco tiene un rango límite, por lo que si tiene valores atípicos en sus datos, no se verán afectados por la estandarización."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d58dcb9b-759d-4015-a218-6e528ab8440a","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Normalización\nLa normalización es una técnica de escalado que a menudo se aplica como parte de la preparación de datos para el aprendizaje automático. El objetivo de la normalización es cambiar los valores de las columnas numéricas en el conjunto de datos para usar una escala común, sin distorsionar las diferencias en los rangos de valores ni perder información. La normalización escala cada variable de entrada numérica por separado al rango [0,1], que es el rango para los valores de coma flotante, donde tenemos la mayor precisión. En otras palabras, los valores de las características se desplazan y reescalan para que terminen oscilando entre 0,00 y 1,00. Esta técnica también se conoce como escalado mínimo-máximo, y Spark proporciona un transformador para este propósito llamado MinMaxScaler.\n\nAquí está la fórmula para la normalización:\n\n$$\\bar X=\\frac{X_i-X_{min}}{X_{max}-X_{min}}$$\n\nTenga en cuenta que $X_{max}$ y $X_{min}$ son los valores máximo y mínimo de la característica dada, $X_i$, respectivamente.\n\nPara ilustrar el proceso de normalización, creemos un DataFrame con tres características:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"40b6fa1b-6b7e-4a74-9162-de5925e0e5dc","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df = spark.createDataFrame([ (100, 77560, 45),(200, 41560, 23),(300, 30285, 20),\n                            (400, 10345, 6),(500, 88000, 50)], \n                           [\"user_id\", \"revenue\",\"num_days\"])\nprint(\"Antes de Scaling :\")\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0462bc50-72ff-456e-ba61-e4b0a0a7c2b4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Antes de Scaling :\n+-------+-------+--------+\n|user_id|revenue|num_days|\n+-------+-------+--------+\n|    100|  77560|      45|\n|    200|  41560|      23|\n|    300|  30285|      20|\n|    400|  10345|       6|\n|    500|  88000|      50|\n+-------+-------+--------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["A continuación, aplicaremos `MinMaxScaler` a nuestras características:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a3f13a1b-95f7-43cc-b93d-dfacccb3e17c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import MinMaxScaler\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import DoubleType\n\n# UDF para convertir el tipo de columna de vector a tipo doble\nunlist = udf(lambda x: round(float(list(x)[0]),3), DoubleType())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2481e3fb-b496-45b6-9644-9290988b2732","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Iterando sobre las columnas a escalar\nfor i in [\"revenue\",\"num_days\"]:\n    # Transformación VectorAssembler: conversión de columna a tipo vectorial\n    assembler = VectorAssembler(inputCols=[i],outputCol=i+\"_Vect\")\n    # MinMaxScaler transformation\n    scaler = MinMaxScaler(inputCol=i+\"_Vect\", outputCol=i+\"_Scaled\")\n    # Pipeline y VectorAssembler y MinMaxScaler\n    pipeline = Pipeline(stages=[assembler, scaler])\n    # Fitting pipeline on DataFrame\ndf = pipeline.fit(df).transform(df).withColumn(i+\"_Scaled\", unlist(i+\"_Scaled\")).drop(i+\"_Vect\")\n    \n\ndf.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"50083b51-c2e6-444b-a937-9bf1481c86d0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+-------+--------+--------------+---------------+\n|user_id|revenue|num_days|revenue_Scaled|num_days_Scaled|\n+-------+-------+--------+--------------+---------------+\n|    100|  77560|      45|         0.866|          0.886|\n|    200|  41560|      23|         0.402|          0.386|\n|    300|  30285|      20|         0.257|          0.318|\n|    400|  10345|       6|           0.0|            0.0|\n|    500|  88000|      50|           1.0|            1.0|\n+-------+-------+--------+--------------+---------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["La normalización es una buena técnica para usar cuando sabe que sus datos no siguen una distribución gaussiana. Esto puede ser útil en algoritmos que no asumen ninguna distribución de los datos, como la regresión lineal, los k vecinos más cercanos y las redes neuronales. En las siguientes secciones, veremos algunos ejemplos más."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9dbaf184-bbf3-4e31-9436-3d76783bd03c","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Scaling a Column Using a Pipeline\nAl igual que con la tokenización, podemos aplicar la normalización en una canalización. Primero, definamos un conjunto de características:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2aef58f7-6fa9-478c-9e07-c7597425763b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import MinMaxScaler\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler\ntriplets = [(0, 1, 100), (1, 2, 200), (2, 5, 1000)]\ndf = spark.createDataFrame(triplets, ['x', 'y', 'z'])\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"76d1bc8c-af82-4f7b-a4fb-1c9fad978459","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+---+----+\n|  x|  y|   z|\n+---+---+----+\n|  0|  1| 100|\n|  1|  2| 200|\n|  2|  5|1000|\n+---+---+----+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["Ahora podemos aplicar MinMaxScaler en una canalización de la siguiente manera para normalizar los valores de la función (columna) $x$:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"79fabe2d-f112-41a7-a526-36a1c1ebf9ae","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["assembler = VectorAssembler(inputCols=[\"x\"], outputCol=\"x_vector\")\nscaler = MinMaxScaler(inputCol=\"x_vector\", outputCol=\"x_scaled\")\npipeline = Pipeline(stages=[assembler, scaler])\nscalerModel = pipeline.fit(df)\nscaledData = scalerModel.transform(df)\nscaledData.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d79a8276-3ee2-41f8-ad39-ad2cf9b6908b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+---+----+--------+--------+\n|x  |y  |z   |x_vector|x_scaled|\n+---+---+----+--------+--------+\n|0  |1  |100 |[0.0]   |[0.0]   |\n|1  |2  |200 |[1.0]   |[0.5]   |\n|2  |5  |1000|[2.0]   |[1.0]   |\n+---+---+----+--------+--------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### Using MinMaxScaler on Multiple Columns\nTambién podemos aplicar un escalador (como MinMaxScaler) en varias columnas:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d1db744b-67da-43b1-9872-e2345569d4bd","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["triplets = [(0, 1, 100), (1, 2, 200), (2, 5, 1000)]\ndf = spark.createDataFrame(triplets, ['x', 'y', 'z'])\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8fb594e8-7ddc-4d4e-90c4-cf45a0a021f7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+---+----+\n|  x|  y|   z|\n+---+---+----+\n|  0|  1| 100|\n|  1|  2| 200|\n|  2|  5|1000|\n+---+---+----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import MinMaxScaler\n\ncolumns_to_scale = [\"x\", \"y\", \"z\"]\nassemblers = [VectorAssembler(inputCols=[col],outputCol=col + \"_vector\") for col in columns_to_scale]\nscalers = [MinMaxScaler(inputCol=col + \"_vector\",outputCol=col + \"_scaled\") for col in columns_to_scale]\npipeline = Pipeline(stages=assemblers + scalers)\nscalerModel = pipeline.fit(df)\nscaledData = scalerModel.transform(df)\nscaledData.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"70ce5506-52c6-4577-bd8e-4201e433535e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+---+----+--------+--------+--------+--------+--------+--------------------+\n|x  |y  |z   |x_vector|y_vector|z_vector|x_scaled|y_scaled|z_scaled            |\n+---+---+----+--------+--------+--------+--------+--------+--------------------+\n|0  |1  |100 |[0.0]   |[1.0]   |[100.0] |[0.0]   |[0.0]   |[0.0]               |\n|1  |2  |200 |[1.0]   |[2.0]   |[200.0] |[0.5]   |[0.25]  |[0.1111111111111111]|\n|2  |5  |1000|[2.0]   |[5.0]   |[1000.0]|[1.0]   |[1.0]   |[1.0]               |\n+---+---+----+--------+--------+--------+--------+--------+--------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["Puede realizar un procesamiento posterior para recuperar los nombres de las columnas originales:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6e501483-d98e-4188-9a4b-890a7767c0dd","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql import functions as f\n\nnames = {x + \"_scaled\": x for x in columns_to_scale}\nscaledData = scaledData.select([f.col(c).alias(names[c]) for c in names.keys()])\nscaledData.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8ca8869d-1b34-48b9-8555-69fbd4fab4bd","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+------+--------------------+\n|    x|     y|                   z|\n+-----+------+--------------------+\n|[0.0]| [0.0]|               [0.0]|\n|[0.5]|[0.25]|[0.1111111111111111]|\n|[1.0]| [1.0]|               [1.0]|\n+-----+------+--------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### Normalización usando Normalizer\nNormalizer es un transformador que transforma un conjunto de datos de filas de vectores, normalizando cada vector para que tenga una norma de unidad. Toma el parámetro p, que especifica la norma p utilizada para la normalización. (p=2 por defecto). Esta normalización puede ayudar a estandarizar sus datos de entrada y mejorar el comportamiento de los algoritmos de aprendizaje.\n\nEl Normalizador de Spark transforma un conjunto de datos de filas de vectores, normalizando cada vector para que tenga una norma de unidad (es decir, una longitud de 1). Toma un parámetro $p$ del usuario, que representa la $p-norma$. Por ejemplo, puede configurar $p=1$ para usar la norma de Manhattan (o la distancia de Manhattan) o $p=2$ para usar la norma euclidiana:\n\n$$L_1: z = || x ||_1 = sum(|x_i|) for i=1, \\cdots, n$$\n\n$$L_2: z = || x ||_2 = sqrt(sum(x_i^2)) for i=1,\\cdots, n$$"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f931cb7f-df75-4af2-aee7-beb7f007c47f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import Normalizer\nfrom pyspark.ml.linalg import Vectors\n\ndata = spark.createDataFrame([\n    (0, Vectors.dense([1.0, 0.5, -1.0]),),\n    (1, Vectors.dense([2.0, 1.0, 1.0]),),\n    (2, Vectors.dense([4.0, 10.0, 2.0]),)\n], [\"id\", \"features\"])\ndata.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8fa84809-b94d-466c-8271-850a0a35c8a5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+--------------+\n| id|      features|\n+---+--------------+\n|  0|[1.0,0.5,-1.0]|\n|  1| [2.0,1.0,1.0]|\n|  2|[4.0,10.0,2.0]|\n+---+--------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# Normalize each Vector using $L^1$ norm.\nnormalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)\nl1NormData = normalizer.transform(data)\nprint(\"Normalized usando la norma L^1\")\nl1NormData.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"66a5ebfb-f407-4f3f-bd69-4bd0f9b1e997","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Normalized usando la norma L^1\n+---+--------------+------------------+\n| id|      features|      normFeatures|\n+---+--------------+------------------+\n|  0|[1.0,0.5,-1.0]|    [0.4,0.2,-0.4]|\n|  1| [2.0,1.0,1.0]|   [0.5,0.25,0.25]|\n|  2|[4.0,10.0,2.0]|[0.25,0.625,0.125]|\n+---+--------------+------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# Normalize each Vector using $L^2$ norm.\nnormalizer = Normalizer(inputCol=\"features\", outputCol=\"norma2\", p=2.0)\nl2NormData = normalizer.transform(data)\nprint(\"Normalizando usando la norma L^2\")\nl2NormData.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"00b33388-e85f-43d3-8ed7-dda1a9ff8830","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Normalizando usando la norma $L^2$\n+---+--------------+--------------------+\n| id|      features|              norma2|\n+---+--------------+--------------------+\n|  0|[1.0,0.5,-1.0]|[0.66666666666666...|\n|  1| [2.0,1.0,1.0]|[0.81649658092772...|\n|  2|[4.0,10.0,2.0]|[0.36514837167011...|\n+---+--------------+--------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# Normalize each Vector using $L^\\infty$ norm.\nlInfNormData = normalizer.transform(data, {normalizer.p: float(\"inf\")})\nprint(\"Normalizando usando la norma L^inf\")\nlInfNormData.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8753d939-8ab2-4560-b07c-f3f29f3fc2d8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Normalizando usando la norma L^inf\n+---+--------------+--------------+\n| id|      features|        norma2|\n+---+--------------+--------------+\n|  0|[1.0,0.5,-1.0]|[1.0,0.5,-1.0]|\n|  1| [2.0,1.0,1.0]| [1.0,0.5,0.5]|\n|  2|[4.0,10.0,2.0]| [0.4,1.0,0.2]|\n+---+--------------+--------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import Normalizer\n# Creando un object de la class Normalizer\nManhattanDistance=Normalizer().setP(1).setInputCol(\"features\").setOutputCol(\"Manhattan Distance\")\n\nEuclideanDistance=Normalizer().setP(2).setInputCol(\"features\").setOutputCol(\"Euclidean Distance\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c5418ea9-64c7-4793-8d4c-918136338815","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## String Indexing (Indexación de cadenas)\nLa mayoría de los algoritmos de **Machine Learning** requieren la conversión de características categóricas (como cadenas) en numéricas. La indexación de cadenas es el proceso de convertir cadenas en valores numéricos.\n\n`StringIndexer` de **Spark** es un indexador de etiquetas que asigna una columna de cadenas de etiquetas a una columna de índices de etiquetas. Si la columna de entrada es numérica, la convertimos en cadena e indexamos los valores de cadena. Los índices están en el rango [0, numLabels). De forma predeterminada, se ordenan por frecuencia de etiqueta en orden descendente, por lo que la etiqueta más frecuente obtiene el índice 0. El comportamiento de ordenación se controla configurando la opción `stringOrderType`.\n\nStringIndexer codifica una columna de cadenas de etiquetas en una columna de índices de etiquetas. StringIndexer puede codificar varias columnas. Los índices están en $[0, numLabels)$ y se admiten cuatro opciones de ordenación: `frequencyDesc`: orden descendente por frecuencia de etiqueta (etiqueta más frecuente asignada 0), `frequencyAsc`: orden ascendente por frecuencia de etiqueta (etiqueta menos frecuente asignada 0) , `alphabetDesc`: orden alfabético descendente y `alphabetAsc`: orden alfabético ascendente (predeterminado = `frequencyDesc`). Tenga en cuenta que en el caso de la misma frecuencia en `frequencyDesc`/`frequencyAsc`, las cadenas se ordenan alfabéticamente.\n\nLas etiquetas invisibles se colocarán en el índice numLabels si el usuario decide conservarlas. Si la columna de entrada es numérica, la convertimos en cadena e indexamos los valores de cadena. Cuando los componentes de pipeline posteriores, como Estimator o `Transformer`, utilizan esta etiqueta indexada por cadena, debe establecer la columna de entrada del componente en este nombre de columna indexada por cadena. En muchos casos, puede configurar la columna de entrada con `setInputCol`."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6fcef710-6d51-4379-bf28-93ce03a0ae70","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df = spark.createDataFrame(\n    [(1111111,20151122045510, \"Yin\",\"gre\"), (1111111,20151122045501, \"Yin\",\"gre\"), (1111111,20151122045500, \"Yln\",\"gra\")\n     , (1111112,20151122065832, \"Yun\",\"ddd\"), (1111113,20160101003221, \"Yan\",\"fdf\"), (1111111,20160703045231, \"Yin\",\"gre\"),\n    (1111114,20150419134543, \"Yin\",\"fdf\"), (1111115,20151123174302, \"Yen\",\"ddd\"),(2111115, 20123192, \"Yen\",\"gre\")],\n    [\"address\", \"date\",\"name\",\"food\"])\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"405edc27-9960-4fc7-b3b6-2b8b9ef986fa","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+--------------+----+----+\n|address|          date|name|food|\n+-------+--------------+----+----+\n|1111111|20151122045510| Yin| gre|\n|1111111|20151122045501| Yin| gre|\n|1111111|20151122045500| Yln| gra|\n|1111112|20151122065832| Yun| ddd|\n|1111113|20160101003221| Yan| fdf|\n|1111111|20160703045231| Yin| gre|\n|1111114|20150419134543| Yin| fdf|\n|1111115|20151123174302| Yen| ddd|\n|2111115|      20123192| Yen| gre|\n+-------+--------------+----+----+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["Si queremos transformarlo para usarlo con `pyspark.ml`, podemos usar `StringIndexer` de Spark para convertir la columna de nombre en una columna numérica, como se muestra aquí:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8005fd01-c33c-4f20-bcf0-6e94673a59a5","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["indexer = StringIndexer(inputCol=\"name\", outputCol=\"name_index\").fit(df)\ndf_ind = indexer.transform(df)\ndf_ind.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"677a6241-faae-49f2-8208-965793d0717c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+--------------+----+----+----------+\n|address|          date|name|food|name_index|\n+-------+--------------+----+----+----------+\n|1111111|20151122045510| Yin| gre|       0.0|\n|1111111|20151122045501| Yin| gre|       0.0|\n|1111111|20151122045500| Yln| gra|       3.0|\n|1111112|20151122065832| Yun| ddd|       4.0|\n|1111113|20160101003221| Yan| fdf|       2.0|\n|1111111|20160703045231| Yin| gre|       0.0|\n|1111114|20150419134543| Yin| fdf|       0.0|\n|1111115|20151123174302| Yen| ddd|       1.0|\n|2111115|      20123192| Yen| gre|       1.0|\n+-------+--------------+----+----+----------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### Aplicar StringIndexer a varias columnas\n¿Qué pasa si queremos aplicar `StringIndexer` a varias columnas a la vez? La manera simple de hacer esto es combinar varios `StringIndexes` en una función `list()`` y usar un Pipeline para ejecutarlos todos:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2327472c-f81d-40a3-b31d-e8c8b0dafeff","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer\n\nindexers = [ StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(df)\n            for column in list(set(df.columns)-set(['date'])) ]\npipeline = Pipeline(stages=indexers)\ndf_indexed = pipeline.fit(df).transform(df)\n\ndf_indexed.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3eb83ea2-debd-4607-9546-8648d1eda921","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------+--------------+----+----+----------+----------+-------------+\n|address|          date|name|food|food_index|name_index|address_index|\n+-------+--------------+----+----+----------+----------+-------------+\n|1111111|20151122045510| Yin| gre|       0.0|       0.0|          0.0|\n|1111111|20151122045501| Yin| gre|       0.0|       0.0|          0.0|\n|1111111|20151122045500| Yln| gra|       3.0|       3.0|          0.0|\n|1111112|20151122065832| Yun| ddd|       1.0|       4.0|          1.0|\n|1111113|20160101003221| Yan| fdf|       2.0|       2.0|          2.0|\n|1111111|20160703045231| Yin| gre|       0.0|       0.0|          0.0|\n|1111114|20150419134543| Yin| fdf|       2.0|       0.0|          3.0|\n|1111115|20151123174302| Yen| ddd|       1.0|       1.0|          4.0|\n|2111115|      20123192| Yen| gre|       0.0|       1.0|          5.0|\n+-------+--------------+----+----+----------+----------+-------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["%md\n## Vector Assembly\nLa función principal de `VectorAssembler` es concatenar un conjunto de características en un solo vector que se puede pasar al estimador o al algoritmo de aprendizaje automático. En otras palabras, es un transformador de funciones que fusiona múltiples columnas en una sola columna vectorial.\n\nTambién `VectorAssembler` es un transformador que combina una lista dada de columnas en una sola columna vectorial. Es útil para combinar características sin procesar y características generadas por diferentes transformadores de características en un solo vector de características, para entrenar modelos ML como regresión logística y árboles de decisión. VectorAssembler acepta los siguientes tipos de columnas de entrada: todos los tipos numéricos, tipo booleano y tipo vectorial. En cada fila, los valores de las columnas de entrada se concatenarán en un vector en el orden especificado."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"54439347-33bd-4195-be73-48f8fc947993","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Veamos el proceso de datos de alto nivel en el modelado de PySpark. Como se muestra en la siguiente esquema. A continuación, comienza con la ingestión de datos seguida del análisis exploratorio de datos, la ingeniería de características, la creación de datos finales y su división en entrenamiento, prueba y validación con fines de modelado. En este artículo, nuestro enfoque es cómo podemos pasar de la ingeniería de características a los pasos de VectorAssembler.\n\n<center><img src=\"https://miro.medium.com/max/1400/1*BkIwDNUo9tnJB_QfxdeC2A.webp \" width=\"550\" height=\"850\"></center>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6f320ba1-d6bb-4d96-bf7b-21c9ee09ca55","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Antes de entrar en los detalles, echemos un vistazo al proceso de alto nivel para ensamblar una variable individual en una columna vectorial de \"características\". En la siguiente figura se muestra que dos variables numéricas X1 y X2 se combinan usando `VectorAssembler` en una sola columna de vector llamada **\"características\"**. \n\n<center><img src=\"https://miro.medium.com/max/720/1*bfI5xK7OZ-_uSJCvtXFPUA.webp\" width=\"550\" height=\"850\"></center>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5e6e8c0c-041c-4570-bdd1-7ac22c6b6c63","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\n\ndataset = spark.createDataFrame(\n    [(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0),(1, 20, 1.0, Vectors.dense([0.0, 15.0, 0.2]), 5.0),\n    (1, 15, 3.0, Vectors.dense([0.0, 11.0, 0.8]), 7.0), (0, 10, 5.0, Vectors.dense([0.3, 11.0, 0.6]), 4.0),\n    (5, 3, 5.0, Vectors.dense([0.7, 19.0, 0.1]), 3.3)],\n    [\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\"])\n\ndataset.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2355f5c2-bd48-484d-9f8b-898176a4edfd","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+----+------+--------------+-------+\n| id|hour|mobile|  userFeatures|clicked|\n+---+----+------+--------------+-------+\n|  0|  18|   1.0|[0.0,10.0,0.5]|    1.0|\n|  1|  20|   1.0|[0.0,15.0,0.2]|    5.0|\n|  1|  15|   3.0|[0.0,11.0,0.8]|    7.0|\n|  0|  10|   5.0|[0.3,11.0,0.6]|    4.0|\n|  5|   3|   5.0|[0.7,19.0,0.1]|    3.3|\n+---+----+------+--------------+-------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["Podemos aplicar `VectorAssembler` a estas tres funciones (id, hour,useFeatures y clicked) y fusionarlas en una columna vectorial denominada features, como se muestra aquí:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4f42836c-e80a-45fd-b92d-1448e49abca6","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["assembler = VectorAssembler(\n    inputCols=[\"hour\", \"mobile\", \"userFeatures\"],\n    outputCol=\"features\")\n\noutput = assembler.transform(dataset)\nprint(\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\")\noutput.select(\"features\", \"clicked\").show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e8743586-5747-4df5-98ec-adaa9cc38890","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\n+-----------------------+-------+\n|features               |clicked|\n+-----------------------+-------+\n|[18.0,1.0,0.0,10.0,0.5]|1.0    |\n|[20.0,1.0,0.0,15.0,0.2]|5.0    |\n|[15.0,3.0,0.0,11.0,0.8]|7.0    |\n|[10.0,5.0,0.3,11.0,0.6]|4.0    |\n|[3.0,5.0,0.7,19.0,0.1] |3.3    |\n+-----------------------+-------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["Si desea omitir filas que tienen **NaN** o valores nulos, puede hacerlo usando `VectorAssembler.setParams(handleInvalid=\"skip\")`:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"af787c07-8211-4fb6-a4cf-f5f2c42d04e4","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["assembler2 = VectorAssembler(inputCols=[\"hour\", \"mobile\", \"userFeatures\"], outputCol=\"features\").setParams(handleInvalid=\"skip\")\nassembler2.transform(dataset).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a8e3467c-1f0f-4022-bf9d-6b5634695d6f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+----+------+--------------+-------+--------------------+\n| id|hour|mobile|  userFeatures|clicked|            features|\n+---+----+------+--------------+-------+--------------------+\n|  0|  18|   1.0|[0.0,10.0,0.5]|    1.0|[18.0,1.0,0.0,10....|\n|  1|  20|   1.0|[0.0,15.0,0.2]|    5.0|[20.0,1.0,0.0,15....|\n|  1|  15|   3.0|[0.0,11.0,0.8]|    7.0|[15.0,3.0,0.0,11....|\n|  0|  10|   5.0|[0.3,11.0,0.6]|    4.0|[10.0,5.0,0.3,11....|\n|  5|   3|   5.0|[0.7,19.0,0.1]|    3.3|[3.0,5.0,0.7,19.0...|\n+---+----+------+--------------+-------+--------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["El modelado en PySpark requiere preparar datos usando `VectorAssembler` que contiene todas las características numéricas y características categóricas convertidas en vectores. `StringIndexer` y `OneHotEncoder` disponibles en `pyspark.ml.feature` son pasos importantes para convertir variables categóricas en una forma vectorizada que luego se puede usar para el trabajo de modelado posterior."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d764705a-cc36-409a-a077-780021870acd","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Bucketing\nData binning— El agrupamiento de datos, también llamado agrupamiento discreto o agrupamiento, es una técnica de preprocesamiento de datos que se utiliza para reducir los efectos de errores de observación menores. Con esta técnica, los valores de datos originales que caen en un pequeño intervalo determinado (un contenedor) se reemplazan por un valor representativo de ese intervalo, a menudo el valor central. Por ejemplo, si tiene datos sobre los precios de los automóviles en los que los valores están muy dispersos, es posible que prefiera utilizar la segmentación en lugar de los precios reales de los automóviles individuales.\n\nEl Bucketizer de Spark transforma una columna de características continuas en una columna de cubos de características, donde el usuario especifica los cubos.\n\nConsidere este ejemplo: no existe una relación lineal entre la latitud y los valores de las viviendas, pero puede sospechar que las latitudes individuales y los valores de las viviendas están relacionados. Para explorar esto, puede clasificar las latitudes en cubos, creando cubos como:\n\nBin-1: 32 < latitude <= 33 \\\nBin-2: 33 < latitude <= 34\n\nLa técnica de binning se puede aplicar tanto en datos categóricos como numéricos. La Tabla 12-2 muestra un ejemplo de clasificación numérica y la Tabla 12-3 muestra un ejemplo de clasificación categórica."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fce41ad1-aa38-4a2d-88e0-665f331e5a73","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Tabla 12-2. Ejemplo de agrupación numérica\n\n|Value| Bin|\n|-----|----|\n|0-10 |Very low|\n|11-30| Low|\n|31-70 |Mid|\n|71-90| High|\n|91-100| Very high|"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eba6f1c0-316a-4286-ad6c-2918bb8f224c","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Tabla 12-3. Ejemplo de agrupación categórica\n|Value| Bin|\n|-----|-----|\n|India| Asia|\n|China| Asia|\n|Japan| Asia|\n|Spain| Europe|\n|Italy| Europe|\n|Chile |South America|\n|Brazil| South America|"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5686cf56-0a4b-4d05-b4c7-7af89e4fe157","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["El agrupamiento también se usa con datos genómicos: agrupamos los cromosomas del genoma humano (1, 2, 3, …, 22, X, Y, MT). Por ejemplo, el cromosoma 1 tiene 250 millones de posiciones, que podemos agrupar en 101 cubos."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5fc40d71-019e-4bb8-baec-37bbfedaa4a9","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Bucketizer\nBucketing es el enfoque más directo para convertir variables continuas en variables categóricas. Para ilustrar, veamos un ejemplo. En PySpark, la tarea de creación de depósitos se puede realizar fácilmente utilizando la clase `Bucketizer`. El primer paso es definir los bordes del cubo; luego creamos un objeto de la clase `Bucketizer` y aplicamos el método `transform()` a nuestro DataFrame.\n\nBucketizer transforma una columna de funciones continuas en una columna de depósitos de funciones, donde los usuarios especifican los depósitos. Toma un parámetro:\n\n* **splits:** Parámetro para mapear entidades continuas en cubos. Con n+1 divisiones, hay n cubos. Un cubo definido por las divisiones x, y contiene valores en el rango [x, y) excepto el último cubo, que también incluye y. Las divisiones deben ser estrictamente crecientes. Los valores en -inf, inf se deben proporcionar explícitamente para cubrir todos los valores de Double; De lo contrario, los valores fuera de las divisiones especificadas se tratarán como errores. Dos ejemplos de divisiones son Array(Double.NegativeInfinity, 0.0, 1.0, Double.PositiveInfinity) y Array(0.0, 1.0, 2.0).\n\nTenga en cuenta que si no tiene idea de los límites superior e inferior de la columna de destino, debe agregar Double.NegativeInfinity y Double.PositiveInfinity como los límites de sus divisiones para evitar una posible excepción fuera de los límites de Bucketizer.\n\nTenga en cuenta también que las divisiones que proporcionó deben estar en orden estrictamente creciente, es decir, $s_0 < s_1 < s_2 < \\cdots < s_n$."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"95e49970-4fdf-48c4-8592-be2f36f3846c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["data = [('A', -99.99), ('B', -0.5), ('C', -0.3),('D', 0.0), ('E', 0.7), ('F', 99.99)]\n\ndataframe = spark.createDataFrame(data, [\"id\", \"features\"])\ndataframe.show()                                                "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"28d00cf9-b0a1-48c2-888c-636f1a19384d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+--------+\n| id|features|\n+---+--------+\n|  A|  -99.99|\n|  B|    -0.5|\n|  C|    -0.3|\n|  D|     0.0|\n|  E|     0.7|\n|  F|   99.99|\n+---+--------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["A continuación, definimos los bordes de nuestros cubos y aplicamos el Bucketizer para crear cubos:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3e98b148-2dce-44f6-bf25-8773104515a7","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import Bucketizer\n\nsplits = [-float(\"inf\"), -0.5, 0.0, 0.5, float(\"inf\")]\n\nbucketizer = Bucketizer(splits=splits, inputCol=\"features\", outputCol=\"bucketedFeatures\")\n\n# Transform original data into its bucket index.\nbucketedData = bucketizer.transform(dataframe)\n\nprint(\"Bucketizer output with %d buckets\" % (len(bucketizer.getSplits()) - 1))\nbucketedData.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"574ca5e4-f315-488d-932e-e7aa2a9c82e0","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Bucketizer output with 4 buckets\n+---+--------+----------------+\n| id|features|bucketedFeatures|\n+---+--------+----------------+\n|  A|  -99.99|             0.0|\n|  B|    -0.5|             1.0|\n|  C|    -0.3|             1.0|\n|  D|     0.0|             2.0|\n|  E|     0.7|             3.0|\n|  F|   99.99|             3.0|\n+---+--------+----------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["## QuantileDiscretizer\nEl `QuantileDiscretizer` de Spark toma una columna con características continuas y genera una columna con características categóricas agrupadas. El número de bandejas se establece mediante el parámetro `numBuckets` y las divisiones de las bandejas se determinan en función de los datos. Es posible que la cantidad de cubos utilizados sea menor que el valor especificado, por ejemplo, si hay muy pocos valores distintos en la entrada para crear suficientes cuantiles distintos (es decir, segmentos del conjunto de datos).\n\nPuede usar `Bucketizer` y `QuantileDiscretizer` juntos, así:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"37364b22-1594-4d44-b03a-5743a987457b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import Bucketizer\nfrom pyspark.ml.feature import QuantileDiscretizer"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"df4910de-9f37-4040-a500-546841aef048","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["data = [(0, 18.0), (1, 19.0), (2, 8.0), (3, 5.0), (4, 2.2)]\ndf = spark.createDataFrame(data, [\"id\", \"hour\"])\nprint(df.show())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9bbe5d3f-e402-4f2d-88dc-777329291f75","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+----+\n| id|hour|\n+---+----+\n|  0|18.0|\n|  1|19.0|\n|  2| 8.0|\n|  3| 5.0|\n|  4| 2.2|\n+---+----+\n\nNone\n"]}],"execution_count":0},{"cell_type":"code","source":["qds = QuantileDiscretizer(numBuckets=5, inputCol=\"hour\",outputCol=\"buckets\", relativeError=0.01, handleInvalid=\"error\")\nbucketizer = qds.fit(df)\nbucketizer.setHandleInvalid(\"skip\").transform(df).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6c5e379c-76ad-4024-9167-2f55b5245082","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+----+-------+\n| id|hour|buckets|\n+---+----+-------+\n|  0|18.0|    4.0|\n|  1|19.0|    4.0|\n|  2| 8.0|    3.0|\n|  3| 5.0|    2.0|\n|  4| 2.2|    1.0|\n+---+----+-------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["## Transformación de logaritmos\nEn pocas palabras, la transformación de logaritmos (comúnmente denotada por log) comprime el rango de números grandes y expande el rango de números pequeños. En matemáticas, el logaritmo es la función inversa de la exponenciación y se define como (donde b se llama el número base):\n\n$$log_b x = y \\rightarrow  b^y = x$$\n\nEn feature engineering, la transformación logarítmica es una de las transformaciones matemáticas más utilizadas. Nos ayuda a manejar datos sesgados forzando los valores atípicos más cerca de la media, haciendo que la distribución de datos se aproxime más a la normalidad (por ejemplo, el logaritmo natural/base e del número 4000 es 8,2940496401). Esta normalización reduce el efecto de los valores atípicos, lo que ayuda a que los modelos de aprendizaje automático sean más sólidos.\n\nEl logaritmo solo se define para valores positivos distintos de 1 (0, 1 y los valores negativos no pueden ser la base de una función de potencia de manera confiable). Una técnica común para manejar valores negativos y cero es agregar una constante a los datos antes de aplicar la transformación logarítmica (por ejemplo, $log(x+1)$)."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9caf9e11-b3ba-49e3-903f-4f7d7aaf09a9","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Spark proporciona la función logarítmica en cualquier base, definida de la siguiente manera:\n\n`pyspark.sql.functions.log(arg1, arg2=None)`\n\n>Descripción: Devuelve el primer logaritmo basado en argumento del segundo argumento. Si solo hay un argumento, entonces este toma el logaritmo natural del argumento. Su uso se ilustra en el siguiente ejemplo. Primero, creamos un DataFrame:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9f2a5da7-ce74-4740-a381-8d98983a8d97","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["data = [('gene1', 1.2), ('gene2', 3.4), ('gene1', 3.5), ('gene2', 12.6)]\ndf = spark.createDataFrame(data, [\"gene\", \"value\"])\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"176da533-1a3a-44a1-81b2-b82c39663970","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+-----+\n| gene|value|\n+-----+-----+\n|gene1|  1.2|\n|gene2|  3.4|\n|gene1|  3.5|\n|gene2| 12.6|\n+-----+-----+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["Luego aplicamos la transformación logarítmica en un valor etiquetado como característica:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b5dc5ce8-ba5f-495f-81bd-3a75599ecaaa","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import log\ndf.withColumn(\"base-10\", log(10.0, df.value)).withColumn(\"base-e\", log(df.value)).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1c29128d-e8eb-4274-a26a-d4fe40764e86","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+-----+------------------+------------------+\n| gene|value|           base-10|            base-e|\n+-----+-----+------------------+------------------+\n|gene1|  1.2|0.0791812460476248|0.1823215567939546|\n|gene2|  3.4| 0.531478917042255|1.2237754316221157|\n|gene1|  3.5|0.5440680443502756| 1.252762968495368|\n|gene2| 12.6|1.1003705451175627| 2.533696813957432|\n+-----+-----+------------------+------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["## One-Hot Encoding\nLos modelos de aprendizaje automático requieren que todas las funciones de entrada y las predicciones de salida sean numéricas. Esto implica que si sus datos contienen características categóricas, como el título de educación {BS, MBA, MS, MD, PHD}, debe codificarlos numéricamente antes de poder construir y evaluar un modelo.\n\nLa figura 12-3 ilustra el concepto de codificación one-hot, un esquema de codificación en el que cada valor categórico se convierte en un vector binario."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1da74c38-9e08-4ed9-b167-cfc2d5404f79","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Un codificador one-hot asigna los índices de etiqueta a una representación vectorial binaria con un único valor 1 como máximo que indica la presencia de un valor de característica específico del conjunto de todos los valores de característica posibles. Este método es útil cuando necesita usar características categóricas pero el algoritmo espera características continuas. Para entender este método de codificación, considere una característica llamada nivel_seguridad que tiene cinco valores categóricos (representados en la Tabla 12-4). La primera columna muestra los valores de las características y el resto de las columnas muestran representaciones vectoriales binarias codificadas en caliente de esos valores."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d11ba610-d935-4240-a6b4-6575a91b22da","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Para los datos de entrada de tipo cadena, es común codificar características categóricas usando `StringIndexer` primero. El OneHotEncoder de Spark luego toma la etiqueta indexada por cadena y la codifica en un vector disperso. Veamos un ejemplo para ver cómo funciona esto. Primero crearemos un DataFrame con dos características categóricas:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d210fd3a-7a5e-4711-aed1-b58ea46b30fb","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.types import *\n\nschema = StructType().add(\"id\",\"integer\").add(\"safety_level\",\"string\").add(\"engine_type\",\"string\")\nschema\n#StructType(list(StructField(id,IntegerType,True),StructField(safety_level,StringType,True),StructField(engine_type,StringType,True)))\n\ndata = [(1,'Very-Low','v4'),(2,'Very-Low','v6'),(3,'Low','v6'),(4,'Low','v6'),(5,'Medium','v4'),\n        (6,'High','v6'),(7,'High','v6'),(8,'Very-High','v4'),(9,'Very-High','v6')]\n\ndf = spark.createDataFrame(data, schema=schema)\ndf.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"03b26fc2-edf6-4dc3-9dd3-f1aed17b9db3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+------------+-----------+\n|id |safety_level|engine_type|\n+---+------------+-----------+\n|1  |Very-Low    |v4         |\n|2  |Very-Low    |v6         |\n|3  |Low         |v6         |\n|4  |Low         |v6         |\n|5  |Medium      |v4         |\n|6  |High        |v6         |\n|7  |High        |v6         |\n|8  |Very-High   |v4         |\n|9  |Very-High   |v6         |\n+---+------------+-----------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"833dabc9-0faa-4c02-9a79-1654639d10fa","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- id: integer (nullable = true)\n |-- safety_level: string (nullable = true)\n |-- engine_type: string (nullable = true)\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["A continuación, aplicaremos la transformación `OneHotEncoder` a las características de nivel de seguridad y tipo de motor. En Spark, no podemos aplicar `OneHotEncoder` directamente a las columnas de cadenas; primero debemos convertirlos a valores numéricos, lo que podemos hacer con `StringIndexer` de Spark.\n\nPrimero, aplicamos `StringIndexer` a la característica de nivel de seguridad:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2a2526dc-9ef0-46fe-a693-049d3861a39e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer\n\nsafety_level_indexer = StringIndexer(inputCol=\"safety_level\",outputCol=\"safety_level_index\")\ndf1 = safety_level_indexer.fit(df).transform(df)\ndf1.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b76e5f38-50e8-4f3e-abe6-7bcab9c0993e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+------------+-----------+------------------+\n| id|safety_level|engine_type|safety_level_index|\n+---+------------+-----------+------------------+\n|  1|    Very-Low|         v4|               3.0|\n|  2|    Very-Low|         v6|               3.0|\n|  3|         Low|         v6|               1.0|\n|  4|         Low|         v6|               1.0|\n|  5|      Medium|         v4|               4.0|\n|  6|        High|         v6|               0.0|\n|  7|        High|         v6|               0.0|\n|  8|   Very-High|         v4|               2.0|\n|  9|   Very-High|         v6|               2.0|\n+---+------------+-----------+------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["A continuación, aplicamos StringIndexer a la característica engine_type:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"73e78e49-cadc-4b86-b086-cd2b847f7c46","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["engine_type_indexer = StringIndexer(inputCol=\"engine_type\",outputCol=\"engine_type_index\")\ndf2 = engine_type_indexer.fit(df).transform(df)\ndf2.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7e11c69d-b8b6-49e4-906c-fa2c73314be9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+------------+-----------+-----------------+\n| id|safety_level|engine_type|engine_type_index|\n+---+------------+-----------+-----------------+\n|  1|    Very-Low|         v4|              1.0|\n|  2|    Very-Low|         v6|              0.0|\n|  3|         Low|         v6|              0.0|\n|  4|         Low|         v6|              0.0|\n|  5|      Medium|         v4|              1.0|\n|  6|        High|         v6|              0.0|\n|  7|        High|         v6|              0.0|\n|  8|   Very-High|         v4|              1.0|\n|  9|   Very-High|         v6|              0.0|\n+---+------------+-----------+-----------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["Ahora podemos aplicar **OneHotEncoder** al safety_level_index y a la columna engine_type_index"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c4ecbec6-2d94-4244-ac79-36a6190e5b34","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import OneHotEncoder\nonehotencoder_safety_level = OneHotEncoder(inputCol=\"safety_level_index\",outputCol=\"safety_level_vector\")\ndf11 = onehotencoder_safety_level.fit(df1).transform(df1)\ndf11.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9a2bba70-0f00-4546-af05-bf088c3dd9f2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+------------+-----------+------------------+-------------------+\n|id |safety_level|engine_type|safety_level_index|safety_level_vector|\n+---+------------+-----------+------------------+-------------------+\n|1  |Very-Low    |v4         |3.0               |(4,[3],[1.0])      |\n|2  |Very-Low    |v6         |3.0               |(4,[3],[1.0])      |\n|3  |Low         |v6         |1.0               |(4,[1],[1.0])      |\n|4  |Low         |v6         |1.0               |(4,[1],[1.0])      |\n|5  |Medium      |v4         |4.0               |(4,[],[])          |\n|6  |High        |v6         |0.0               |(4,[0],[1.0])      |\n|7  |High        |v6         |0.0               |(4,[0],[1.0])      |\n|8  |Very-High   |v4         |2.0               |(4,[2],[1.0])      |\n|9  |Very-High   |v6         |2.0               |(4,[2],[1.0])      |\n+---+------------+-----------+------------------+-------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["onehotencoder_engine_type = OneHotEncoder(inputCol=\"engine_type_index\",outputCol=\"engine_type_vector\")\ndf12 = onehotencoder_engine_type.fit(df2).transform(df2)\ndf12.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a76dc496-1673-4b95-b2ff-8c5c98289e5f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+------------+-----------+-----------------+------------------+\n|id |safety_level|engine_type|engine_type_index|engine_type_vector|\n+---+------------+-----------+-----------------+------------------+\n|1  |Very-Low    |v4         |1.0              |(1,[],[])         |\n|2  |Very-Low    |v6         |0.0              |(1,[0],[1.0])     |\n|3  |Low         |v6         |0.0              |(1,[0],[1.0])     |\n|4  |Low         |v6         |0.0              |(1,[0],[1.0])     |\n|5  |Medium      |v4         |1.0              |(1,[],[])         |\n|6  |High        |v6         |0.0              |(1,[0],[1.0])     |\n|7  |High        |v6         |0.0              |(1,[0],[1.0])     |\n|8  |Very-High   |v4         |1.0              |(1,[],[])         |\n|9  |Very-High   |v6         |0.0              |(1,[0],[1.0])     |\n+---+------------+-----------+-----------------+------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["También podemos aplicar esta codificación a varias columnas al mismo tiempo:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"44c53891-71eb-4ad1-b583-9f9dada487cf","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(df) for column in list(set(df.columns)-set(['id'])) ]\n\nfrom pyspark.ml import Pipeline\n\npipeline = Pipeline(stages=indexers)\ndf_indexed = pipeline.fit(df).transform(df)\ndf_indexed.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a22f808f-dba9-4d16-b609-2eb680f8b126","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+------------+-----------+------------------+-----------------+\n| id|safety_level|engine_type|safety_level_index|engine_type_index|\n+---+------------+-----------+------------------+-----------------+\n|  1|    Very-Low|         v4|               3.0|              1.0|\n|  2|    Very-Low|         v6|               3.0|              0.0|\n|  3|         Low|         v6|               1.0|              0.0|\n|  4|         Low|         v6|               1.0|              0.0|\n|  5|      Medium|         v4|               4.0|              1.0|\n|  6|        High|         v6|               0.0|              0.0|\n|  7|        High|         v6|               0.0|              0.0|\n|  8|   Very-High|         v4|               2.0|              1.0|\n|  9|   Very-High|         v6|               2.0|              0.0|\n+---+------------+-----------+------------------+-----------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["encoder = OneHotEncoder(inputCols=[indexer.getOutputCol() for indexer in indexers],\n                        outputCols=[\"{0}_encoded\".format(indexer.getOutputCol()) for indexer in indexers])\n\nfrom pyspark.ml.feature import VectorAssembler\n\nassembler = VectorAssembler(inputCols=encoder.getOutputCols(),outputCol=\"features\")\n\npipeline = Pipeline(stages=indexers + [encoder, assembler])\n\npipeline.fit(df).transform(df).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"66f31ad3-eccd-4d00-97a4-387d488cbfee","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+------------+-----------+------------------+-----------------+--------------------------+-------------------------+-------------------+\n| id|safety_level|engine_type|safety_level_index|engine_type_index|safety_level_index_encoded|engine_type_index_encoded|           features|\n+---+------------+-----------+------------------+-----------------+--------------------------+-------------------------+-------------------+\n|  1|    Very-Low|         v4|               3.0|              1.0|             (4,[3],[1.0])|                (1,[],[])|      (5,[3],[1.0])|\n|  2|    Very-Low|         v6|               3.0|              0.0|             (4,[3],[1.0])|            (1,[0],[1.0])|(5,[3,4],[1.0,1.0])|\n|  3|         Low|         v6|               1.0|              0.0|             (4,[1],[1.0])|            (1,[0],[1.0])|(5,[1,4],[1.0,1.0])|\n|  4|         Low|         v6|               1.0|              0.0|             (4,[1],[1.0])|            (1,[0],[1.0])|(5,[1,4],[1.0,1.0])|\n|  5|      Medium|         v4|               4.0|              1.0|                 (4,[],[])|                (1,[],[])|          (5,[],[])|\n|  6|        High|         v6|               0.0|              0.0|             (4,[0],[1.0])|            (1,[0],[1.0])|(5,[0,4],[1.0,1.0])|\n|  7|        High|         v6|               0.0|              0.0|             (4,[0],[1.0])|            (1,[0],[1.0])|(5,[0,4],[1.0,1.0])|\n|  8|   Very-High|         v4|               2.0|              1.0|             (4,[2],[1.0])|                (1,[],[])|      (5,[2],[1.0])|\n|  9|   Very-High|         v6|               2.0|              0.0|             (4,[2],[1.0])|            (1,[0],[1.0])|(5,[2,4],[1.0,1.0])|\n+---+------------+-----------+------------------+-----------------+--------------------------+-------------------------+-------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["Hay otra forma de hacer todas las transformaciones de datos: podemos usar un pipeline para simplificar el proceso. Primero, creamos las etapas requeridas:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8d990474-f014-460e-ad79-3bc6f6b0c764","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["safety_level_indexer = StringIndexer(inputCol=\"safety_level\",outputCol=\"safety_level_index\")\nengine_type_indexer = StringIndexer(inputCol=\"engine_type\", outputCol=\"engine_type_index\")\nonehotencoder_safety_level = OneHotEncoder(inputCol=\"safety_level_index\",outputCol=\"safety_level_vector\")\nonehotencoder_engine_type = OneHotEncoder(inputCol=\"engine_type_index\",outputCol=\"engine_type_vector\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7752ba5c-86d6-46dc-9f7a-08d09b08f302","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Luego creamos un pipeline y le pasamos todas las etapas definidas:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bafe4ad1-8ec9-488e-a436-147e3fe83cd2","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["pipeline = Pipeline(stages=[safety_level_indexer,\n                            engine_type_indexer, onehotencoder_safety_level, onehotencoder_engine_type])\n\ndf_transformed = pipeline.fit(df).transform(df)\ndf_transformed.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5cc509cf-bc0e-4edc-b30d-101c83989bb8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+------------+-----------+------------------+-----------------+-------------------+------------------+\n|id |safety_level|engine_type|safety_level_index|engine_type_index|safety_level_vector|engine_type_vector|\n+---+------------+-----------+------------------+-----------------+-------------------+------------------+\n|1  |Very-Low    |v4         |3.0               |1.0              |(4,[3],[1.0])      |(1,[],[])         |\n|2  |Very-Low    |v6         |3.0               |0.0              |(4,[3],[1.0])      |(1,[0],[1.0])     |\n|3  |Low         |v6         |1.0               |0.0              |(4,[1],[1.0])      |(1,[0],[1.0])     |\n|4  |Low         |v6         |1.0               |0.0              |(4,[1],[1.0])      |(1,[0],[1.0])     |\n|5  |Medium      |v4         |4.0               |1.0              |(4,[],[])          |(1,[],[])         |\n|6  |High        |v6         |0.0               |0.0              |(4,[0],[1.0])      |(1,[0],[1.0])     |\n|7  |High        |v6         |0.0               |0.0              |(4,[0],[1.0])      |(1,[0],[1.0])     |\n|8  |Very-High   |v4         |2.0               |1.0              |(4,[2],[1.0])      |(1,[],[])         |\n|9  |Very-High   |v6         |2.0               |0.0              |(4,[2],[1.0])      |(1,[0],[1.0])     |\n+---+------------+-----------+------------------+-----------------+-------------------+------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["## TF-IDF\nTerm frequency–inverse document frequency (TF-IDF) is a measure of the originality of a word (a.k.a. term) based on the number of times it appears in a document and the number of documents in a collection that it appears in. In other words, it’s a feature vectorization method used in text mining to reflect the importance of a term to a document in a corpus (set of documents). The TF-IDF technique is commonly used in document analysis, search engines, recommender systems, and other natural language processing (NLP) applications.\n\nTerm frequency TF(t,d) is the number of times that term t appears in document d, while document frequency DF(t, D) is the number of documents that contain term t. If a term appears very often across the corpus, it means it does not carry special information about a particular document—usually these kinds of words (such as “of,” “the,” and “as”) may be dropped from the text analysis. Before we go deeper into the TF-IDF transformation, let’s define the terms used in the following equations (Table 12-5).\n\nTable 12-5. TF-IDF notation\n\n|Notation| Description|\n|--------|------------|\n|t| Term|\n|d| Document|\n|D| Corpus (set of finite documents)|\n|D| The number of documents in the corpus|\n|TF(t, d)| Term Frequency: the number of times that term t appears in document d|\n|DF(t, D)| Document Frequency: the number of documents that contain term t|\n|IDF(t, D)| Inverse Document Frequency: a numerical measure of how much information a term provides|"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"41d24048-c0f7-4c57-a2d2-805dc88f64fb","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["La frecuencia de documento inversa (IDF) se define como:\n\n$$IDF(t,D)=log(\\frac{|D|+1} {DF(t,D)+1}) $$\n\nDigamos que $N$ es el número de documentos en un corpus. Dado que se utiliza el logaritmo, si un término aparece en todos los documentos, su valor IDF se convierte en 0:\n\n$$IDF (t,D) = log \\frac{N + 1}{N + 1} = log 1 = 0$$\n\nTenga en cuenta que se aplica un término de suavizado (+1) para evitar dividir por cero los términos que no aparecen en el corpus. La medida TF-IDF es simplemente el producto de TF e IDF:\n\n$$TF − IDF (t, d,D) = TF (t, d) \\times  IDF (t,D)$$\n\ndónde:\n* t denota el(los) término(s)\n* d denota un documento\n* D denota el corpus\n* TF(t,d) denota el número de veces que el término t aparece en el documento d"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"20eb656e-eb37-46e6-8159-00b587fcbe16","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["Nosotros podemos expresar TF como:\n\n$$TF_{i,j}= \\frac{n_{i,j}{\\sum_k n_{k,j}} IDF_i =log \\frac{|D|}{d:t_i \\in d}$$\n\nAntes, le muestro cómo Spark implementa TF-IDF, veamos un ejemplo simple con dos documentos (el tamaño del corpus es 2 y D = {doc1, doc2}). Comenzamos calculando la frecuencia del término y la frecuencia del documento:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"728d486b-92f7-4f4e-843d-86ba707f6a2a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["documents = spark.createDataFrame([(\"doc1\", \"Ada Ada Spark Spark Spark\"),(\"doc2\", \"Ada SQL\")],[\"id\", \"document\"])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"573f62d3-6830-453b-8e2f-04de5e07cbb6","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["TF(Ada, doc1) = 2\nTF(Spark, doc1) = 3\nTF(Ada, doc2) = 1\nTF(SQL, doc2) = 1\nDF(Ada, D) = 2\nDF(Spark, D) = 1\nDF(SQL, D) = 1"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"efae5153-7ad5-4105-a89d-0f0520aa3d4a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;36m  File \u001B[0;32m\"<command-1716065181506094>\"\u001B[0;36m, line \u001B[0;32m1\u001B[0m\n\u001B[0;31m    TF(Ada, doc1) = 2\u001B[0m\n\u001B[0m    ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m cannot assign to function call\n","errorSummary":"<span class='ansi-red-fg'>SyntaxError</span>: cannot assign to function call (<command-1716065181506094>, line 1)","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;36m  File \u001B[0;32m\"<command-1716065181506094>\"\u001B[0;36m, line \u001B[0;32m1\u001B[0m\n","\u001B[0;31m    TF(Ada, doc1) = 2\u001B[0m\n","\u001B[0m    ^\u001B[0m\n","\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m cannot assign to function call\n"]}}],"execution_count":0},{"cell_type":"markdown","source":["Luego calculamos el IDF y el TF-IDF (tenga en cuenta que la base del logaritmo es e para todos los cálculos):"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e30a0a14-a1ca-4ca0-8d61-03217b5308ed","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["IDF(Ada, D) = log ( (|D|+1) / (DF(t,D)+1) )\n= log ( (2+1) / (DF(Ada, D)+1) )\n= log ( 3 / (2+1)) = log(1)\n= 0.00\n\nIDF(Spark, D) = log ( (|D|+1) / (DF(t,D)+1) )\n= log ( (2+1) / (DF(Spark, D)+1) )\n= log ( 3 / (1+1) )\n= log (1.5)\n= 0.40546510811\n\nTF-IDF(Ada, doc1, D) = TF(Ada, doc1) x IDF(Ada, D)\n= 2 x 0.0\n= 0.0\n\nTF-IDF(Spark, doc1, D) = TF(Spark, doc1) x IDF(Spark, D)\n= 3 x 0.40546510811\n= 1.21639532433"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"60231289-e078-4967-87dd-c2924dcbc10c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["En Spark, HashingTF y CountVectorizer son los dos algoritmos utilizados para generar vectores de frecuencia de términos. El siguiente ejemplo muestra cómo realizar las transformaciones requeridas. Primero, creamos nuestro DataFrame de muestra:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8d015cc4-3585-4916-ab05-e312a59fb40c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n\nsentences = spark.createDataFrame([ (0.0, \"we heard about Spark and Java\"),(0.0, \"Does Java use case classes\"),\n                                   (1.0, \"fox jumped over fence\"),(1.0, \"red fox jumped over\")], [\"label\", \"text\"])\n\nsentences.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ef230929-5f1e-41bf-91a7-2fc0565f0e3b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+-----------------------------+\n|label|text                         |\n+-----+-----------------------------+\n|0.0  |we heard about Spark and Java|\n|0.0  |Does Java use case classes   |\n|1.0  |fox jumped over fence        |\n|1.0  |red fox jumped over          |\n+-----+-----------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\nwords_data = tokenizer.transform(sentences)\nwords_data.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fbda779a-a187-4f35-909d-7de74a1c5d18","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+-----------------------------+------------------------------------+\n|label|text                         |words                               |\n+-----+-----------------------------+------------------------------------+\n|0.0  |we heard about Spark and Java|[we, heard, about, spark, and, java]|\n|0.0  |Does Java use case classes   |[does, java, use, case, classes]    |\n|1.0  |fox jumped over fence        |[fox, jumped, over, fence]          |\n|1.0  |red fox jumped over          |[red, fox, jumped, over]            |\n+-----+-----------------------------+------------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["A continuación, creamos características sin procesar:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"260f0fe6-2b1a-4f9d-9f83-527fb66d0e0c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["hashingTF = HashingTF(inputCol=\"words\", outputCol=\"raw_features\",numFeatures=16)\nfeaturized_data = hashingTF.transform(words_data)\nfeaturized_data.select(\"label\", \"raw_features\").show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f11ac5be-cb6b-40eb-827d-4cc535fb5115","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+-----------------------------------------------+\n|label|raw_features                                   |\n+-----+-----------------------------------------------+\n|0.0  |(16,[1,4,6,11,12,15],[1.0,1.0,1.0,1.0,1.0,1.0])|\n|0.0  |(16,[2,6,11,13,15],[1.0,1.0,1.0,1.0,1.0])      |\n|1.0  |(16,[0,1,6,8],[1.0,1.0,1.0,1.0])               |\n|1.0  |(16,[1,4,6,8],[1.0,1.0,1.0,1.0])               |\n+-----+-----------------------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["Luego aplicamos la transformación IDF():"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5b360330-c84f-4dcd-bd33-ae9eba0e5dfd","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\nidf_model = idf.fit(featurized_data)\nrescaled_data = idf_model.transform(featurized_data)\nrescaled_data.select(\"label\", \"features\").show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cc3170d5-47b1-4004-9dcc-a5d17a5feec5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+---------------------------------------------------------------------------------------------------------------------------+\n|label|features                                                                                                                   |\n+-----+---------------------------------------------------------------------------------------------------------------------------+\n|0.0  |(16,[1,4,6,11,12,15],[0.22314355131420976,0.5108256237659907,0.0,0.5108256237659907,0.9162907318741551,0.5108256237659907])|\n|0.0  |(16,[2,6,11,13,15],[0.9162907318741551,0.0,0.5108256237659907,0.9162907318741551,0.5108256237659907])                      |\n|1.0  |(16,[0,1,6,8],[0.9162907318741551,0.22314355131420976,0.0,0.5108256237659907])                                             |\n|1.0  |(16,[1,4,6,8],[0.22314355131420976,0.5108256237659907,0.0,0.5108256237659907])                                             |\n+-----+---------------------------------------------------------------------------------------------------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["El siguiente ejemplo muestra cómo hacer TF-IDF utilizando CountVectorizer, que extrae un vocabulario de una colección de documentos y genera un CountVectorizerModel. En este ejemplo, cada fila del DataFrame representa un documento:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3519ae10-a539-4f96-afd1-6abfcfac448e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df = spark.createDataFrame([(0, [\"a\", \"b\", \"c\"]), (1, [\"a\", \"b\", \"b\", \"c\", \"a\"])], [\"label\", \"raw\"] )\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6f2b415d-2f5e-4076-add9-ae6bb5c976db","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+---------------+\n|label|            raw|\n+-----+---------------+\n|    0|      [a, b, c]|\n|    1|[a, b, b, c, a]|\n+-----+---------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import CountVectorizer\ncv = CountVectorizer().setInputCol(\"raw\").setOutputCol(\"features\")\nmodel = cv.fit(df)\ntransformed = model.transform(df)\ntransformed.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"919d4ff7-9a1d-4278-bd2a-4aa6a672a2f2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+---------------+-------------------------+\n|label|raw            |features                 |\n+-----+---------------+-------------------------+\n|0    |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n|1    |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n+-----+---------------+-------------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["En la columna de características, tomando el ejemplo de la segunda fila:\n* 3 es la longitud del vector.\n* [0, 1, 2] son los índices vectoriales (índice(a)=0, índice(b)=1, índice(c)=2).\n* [2.0,2.0,1.0] son los valores vectoriales.\n\nHashingTF() convierte documentos en vectores de tamaño fijo:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9f29aac8-4455-48b3-bb1f-18cabe7ce888","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["hashing_TF = HashingTF(inputCol=\"raw\", outputCol=\"features\", numFeatures=128)\nresult = hashing_TF.transform(df)\nresult.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"01f2d908-53b6-4899-ab92-ca9d699bf20d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+---------------+-------------------------------+\n|label|raw            |features                       |\n+-----+---------------+-------------------------------+\n|0    |[a, b, c]      |(128,[40,99,117],[1.0,1.0,1.0])|\n|1    |[a, b, b, c, a]|(128,[40,99,117],[1.0,2.0,2.0])|\n+-----+---------------+-------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["Tenga en cuenta que el tamaño del vector generado a través de CountVectorizer depende del corpus de entrenamiento y del documento, mientras que el generado a través de HashingTF tiene un tamaño fijo (lo configuramos en 128). Esto significa que cuando se usa CountVectorizer, cada función sin procesar se asigna a un índice, pero HashingTF puede sufrir colisiones de hash, donde dos o más términos se asignan al mismo índice. Para evitar esto, podemos aumentar la dimensión de la característica de destino."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7f56fb5a-4ea4-4c54-b182-4231afb5f2ce","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Word2Vec\n`Word2Vec` es un Estimador que toma secuencias de palabras que representan documentos y entrena un `Word2VecModel`. El modelo asigna cada palabra a un vector único de tamaño fijo. Word2VecModel transforma cada documento en un vector usando el promedio de todas las palabras en el documento; este vector se puede usar como características para predicción, cálculos de similitud de documentos, etc. Consulte la guía del usuario de MLlib en `Word2Vec` para obtener más detalles.\n\nEn el siguiente ejemplo de código, comenzamos con un conjunto de documentos, cada uno de los cuales se representa como una secuencia de palabras. Para cada documento, lo transformamos en un vector de características. Este vector de características podría luego pasarse a un algoritmo de aprendizaje."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d8b359fa-d00c-4569-8676-a6bbeeeeb1af","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import Word2Vec\n\n# Input data: Each row is a bag of words from a sentence or document.\ndocumentDF = spark.createDataFrame([\n    (\"Hi I heard about Spark\".split(\" \"), ),\n    (\"I wish Java could use case classes\".split(\" \"), ),\n    (\"Logistic regression models are neat\".split(\" \"), )\n], [\"text\"])\n\ndocumentDF.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eeddca4e-e9db-4f87-ab0d-784cf02116dd","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------+\n|                text|\n+--------------------+\n|[Hi, I, heard, ab...|\n|[I, wish, Java, c...|\n|[Logistic, regres...|\n+--------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# Learn a mapping from words to Vectors.\nword2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\nmodel = word2Vec.fit(documentDF)\n\nresult = model.transform(documentDF)\nfor row in result.collect():\n    text, vector = row\n    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"10f57bbb-d8e9-4a3b-8ce0-d250c2a658d7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Text: [Hi, I, heard, about, Spark] => \nVector: [0.012264367192983627,-0.06442034244537354,-0.007622340321540833]\n\nText: [I, wish, Java, could, use, case, classes] => \nVector: [0.05160687722465289,0.025969027541577816,0.02736483487699713]\n\nText: [Logistic, regression, models, are, neat] => \nVector: [-0.06564115285873413,0.02060299552977085,-0.08455150425434113]\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["## ContarVectorizador\n`CountVectorizer` y `CountVectorizerModel` tienen como objetivo ayudar a convertir una colección de documentos de texto en vectores de recuentos de tokens. Cuando no se dispone de un diccionario a priori, `CountVectorizer` se puede utilizar como Estimador para extraer el vocabulario y genera un `CountVectorizerModel`. El modelo produce representaciones dispersas para los documentos sobre el vocabulario, que luego se pueden pasar a otros algoritmos como LDA.\n\nDurante el proceso de ajuste, `CountVectorizer` seleccionará las principales palabras de VocabSize ordenadas por frecuencia de términos en todo el corpus. Un parámetro opcional minDF también afecta el proceso de ajuste al especificar el número mínimo (o fracción si < 1.0) de documentos en los que debe aparecer un término para ser incluido en el vocabulario. Otro parámetro de alternancia binaria opcional controla el vector de salida. Si se establece en verdadero, todos los recuentos distintos de cero se establecen en 1. Esto es especialmente útil para modelos probabilísticos discretos que modelan recuentos binarios, en lugar de enteros."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dd3b149d-7c7f-4936-9a4c-1c6e138f1a47","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import CountVectorizer\n\n# Input data: Each row is a bag of words with a ID.\ndf = spark.createDataFrame([\n    (0, \"a b c\".split(\" \")),\n    (1, \"a b b c a\".split(\" \"))\n], [\"id\", \"words\"])\n\n# fit a CountVectorizerModel from the corpus.\ncv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=3, minDF=2.0)\n\nmodel = cv.fit(df)\n\nresult = model.transform(df)\nresult.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e4a6df68-8bf9-4aa5-906d-b466b2fd6636","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+---------------+-------------------------+\n|id |words          |features                 |\n+---+---------------+-------------------------+\n|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n+---+---------------+-------------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["## FeatureHasher\nEl hashing de características proyecta un conjunto de características categóricas o numéricas en un vector de características de dimensión específica (normalmente sustancialmente más pequeño que el del espacio de características original). Se utiliza un truco de hashing para asignar características a índices en el vector de características. FeatureHasher de Spark opera en múltiples columnas, que pueden contener características numéricas o categóricas. Para las características numéricas, el hash del nombre de la columna es FeatureHasher El hash de características proyecta un conjunto de características categóricas o numéricas en un vector de características de dimensión específica (normalmente, sustancialmente más pequeño que el del espacio de características original). Se utiliza un truco de hashing para asignar características a índices en el vector de características.\n\nFeatureHasher de Spark opera en múltiples columnas, que pueden contener características numéricas o categóricas. Para funciones numéricas, el hash del nombre de la columna se usa para asignar el valor de la función a su índice en el vector de funciones. Para características categóricas y booleanas, se utiliza el hash de la cadena \"column_name=value\", con un valor de indicador de 1.0. Aquí hay un ejemplo:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f704f8e2-c3e2-49d1-93a3-eae4072e7ace","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["El hashing de características proyecta un conjunto de características categóricas o numéricas en un vector de características de dimensión específica (normalmente sustancialmente más pequeño que el del espacio de características original). Esto se hace usando el truco de hashing para mapear características a índices en el vector de características.\n\nEl transformador FeatureHasher opera en varias columnas. Cada columna puede contener características numéricas o categóricas. El comportamiento y manejo de los tipos de datos de columna es el siguiente:\n\n* **Columnas numéricas:** para las funciones numéricas, el valor hash del nombre de la columna se usa para asignar el valor de la función a su índice en el vector de funciones. De forma predeterminada, las características numéricas no se tratan como categóricas (incluso cuando son números enteros). Para tratarlos como categóricos, especifique las columnas relevantes mediante el parámetro categoricalCols.\n* **Columnas de cadena(string):** para características categóricas, el valor hash de la cadena \"column_name=value\" se usa para mapear el índice vectorial, con un valor de indicador de 1.0. Por lo tanto, las características categóricas están codificadas en \"one-hot\" (de manera similar al uso de OneHotEncoder con dropLast=false).\n* **Columnas booleanas:** los valores booleanos se tratan de la misma manera que las columnas de cadenas. Es decir, las características booleanas se representan como \"column_name=true\" o \"column_name=false\", con un valor de indicador de 1,0.\nLos valores nulos (ausentes) se ignoran (implícitamente cero en el vector de características resultante).\n\nLa función hash utilizada aquí también es MurmurHash 3 utilizada en HashingTF. Dado que se usa un módulo simple en el valor hash para determinar el índice del vector, es recomendable usar una potencia de dos como el parámetro numFeatures; de lo contrario, las características no se asignarán uniformemente a los índices vectoriales."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e51b0d63-e55b-4735-8df9-c258a022a2da","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import FeatureHasher\ndf = spark.createDataFrame([(2.1, True, \"1\", \"fox\"), (2.1, False, \"2\", \"gray\"), (3.3, False, \"2\", \"red\"),\n                            (4.4, True, \"4\", \"fox\")], [\"number\", \"boolean\", \"string_number\", \"string\"])\n\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"96a7bbdf-78e7-4aec-9fbe-648ede0b7c53","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------+-------+-------------+------+\n|number|boolean|string_number|string|\n+------+-------+-------------+------+\n|   2.1|   true|            1|   fox|\n|   2.1|  false|            2|  gray|\n|   3.3|  false|            2|   red|\n|   4.4|   true|            4|   fox|\n+------+-------+-------------+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["input_columns = [\"number\", \"boolean\", \"string_number\", \"string\"]\nhasher = FeatureHasher(inputCols=input_columns, outputCol=\"features\")\n#hasher.setInputCols(input_columns)\nfeaturized = hasher.transform(df)\nfeaturized.show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ef4bd89f-75f3-446e-9f64-9cc2e893a653","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------+-------+-------------+------+--------------------------------------------------------+\n|number|boolean|string_number|string|features                                                |\n+------+-------+-------------+------+--------------------------------------------------------+\n|2.1   |true   |1            |fox   |(262144,[102440,112150,135239,185244],[1.0,1.0,2.1,1.0])|\n|2.1   |false  |2            |gray  |(262144,[43117,93531,135239,210818],[1.0,1.0,2.1,1.0])  |\n|3.3   |false  |2            |red   |(262144,[93531,110541,135239,210818],[1.0,1.0,3.3,1.0]) |\n|4.4   |true   |4            |fox   |(262144,[75860,102440,135239,185244],[1.0,1.0,4.4,1.0]) |\n+------+-------+-------------+------+--------------------------------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["## Transformador SQL\nSQLTransformer de Spark implementa las transformaciones definidas por una instrucción SQL. En lugar de registrar su DataFrame como una tabla y luego consultar la tabla, puede aplicar directamente las transformaciones de SQL a sus datos representados como DataFrame. Actualmente, SQLTransformer tiene una funcionalidad limitada y se puede aplicar a un solo DataFrame como __ THIS __, que representa la tabla subyacente del conjunto de datos de entrada.\n\nSQLTransformer admite declaraciones como:\n* SELECT a, a + b AS a_b FROM __ THIS __\n* SELECT a, SQRT(b) AS b_sqrt FROM _ THIS _ where a > 5\n* SELECT a, b, SUM(c) AS c_sum FROM __ THIS __ GROUP BY a, b"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5978a79e-f27a-4c64-94f7-abd44c7a7922","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# SQLTransformer admite declaraciones como:\n\nSELECT salary, salary * 0.06 AS bonus\n    FROM __THIS__\n        WHERE salary > 10000\n\nSELECT dept, location, SUM(salary) AS sum_of_salary\n    FROM __THIS__\n        GROUP BY dept, location"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b3f2986b-8a89-4d18-aca7-3390d63c6c64","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import SQLTransformer\n>>> df = spark.createDataFrame([(10, \"d1\", 27000),(20, \"d1\", 29000),\n                                (40, \"d2\", 31000),(50, \"d2\", 39000)], \n                               [\"id\", \"dept\", \"salary\"])\n\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cc76257a-0b28-45fa-9f7b-8c6d7adf7837","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+----+------+\n| id|dept|salary|\n+---+----+------+\n| 10|  d1| 27000|\n| 20|  d1| 29000|\n| 40|  d2| 31000|\n| 50|  d2| 39000|\n+---+----+------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["query = \"SELECT dept, SUM(salary) AS sum_of_salary FROM __THIS__ GROUP BY dept\"\nsqlTrans = SQLTransformer(statement=query)\nsqlTrans.transform(df).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"708874b1-0f66-411c-bbc8-9d443ecb5888","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+-------------+\n|dept|sum_of_salary|\n+----+-------------+\n|  d1|        56000|\n|  d2|        70000|\n+----+-------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["## Resumen\nEl objetivo de los algoritmos de aprendizaje automático es utilizar datos de entrada para crear modelos utilizables que puedan ayudarnos a responder preguntas. Los datos de entrada comprenden características (como el nivel educativo, el precio del automóvil, el nivel de glucosa, etc.) que se encuentran en forma de columnas estructuradas. En la mayoría de los casos, los algoritmos requieren características con algunas características específicas para funcionar correctamente, lo que plantea la necesidad de ingeniería de características. La biblioteca de aprendizaje automático de Spark, MLlib (incluida en PySpark), tiene un conjunto de API de alto nivel que hacen posible la ingeniería de funciones. La ingeniería de características adecuada ayuda a construir modelos de aprendizaje automático semánticamente adecuados y correctos."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eccdbaaa-526a-4349-b7c9-c5d186df8431","inputWidgets":{},"title":""}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e0cf0a9f-c2f3-426a-8a3e-8438946da10e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["https://spark.apache.org/docs/latest/ml-features#feature-extractors"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"69afb50e-4bf1-4c51-91b9-bdb0d5c3ae5d","inputWidgets":{},"title":""}}}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.10.4","nbconvert_exporter":"python","file_extension":".py"},"application/vnd.databricks.v1+notebook":{"notebookName":"Feature engineering con Pyspark","dashboards":[{"elements":[{"elementNUID":"6d4c6c5f-9909-4a2b-938b-9536fe1b21c4","dashboardResultIndex":0,"guid":"bbbd37be-1697-404d-aa0a-dc57c3f5a946","resultIndex":null,"options":null,"position":{"x":0,"y":0,"height":6,"width":12,"z":null},"elementType":"command"}],"guid":"098fd748-9ea4-49a3-b14e-51dd0b2bb59d","layoutOption":{"stack":true,"grid":true},"version":"DashboardViewV1","nuid":"0ac34e6b-f925-4316-bf67-58ffcf0de13a","origId":1220335974965034,"title":"Untitled","width":1024,"globalVars":{}}],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":51230823672439}},"nbformat":4,"nbformat_minor":0}
